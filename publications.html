<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/3876360bdb005eab.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3876360bdb005eab.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-c7921d34e2dc75c3.js" defer=""></script><script src="/_next/static/chunks/framework-00b57966872fc495.js" defer=""></script><script src="/_next/static/chunks/main-1452d81522d66bc9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-27945e63718aff40.js" defer=""></script><script src="/_next/static/chunks/656-0eb3433604e264e5.js" defer=""></script><script src="/_next/static/chunks/pages/publications-2d71c1e066079704.js" defer=""></script><script src="/_next/static/tfOEPY2JoVhOrpBumd1g3/_buildManifest.js" defer=""></script><script src="/_next/static/tfOEPY2JoVhOrpBumd1g3/_ssgManifest.js" defer=""></script><script src="/_next/static/tfOEPY2JoVhOrpBumd1g3/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen bg-slate-700"><header class="bg-gray-800 mb-8 py-1 text-gray-300"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous"/><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script><div class="container mx-auto flex flex-wrap space-x-12 items-center justify-center"><button class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center font-semibold text-lg">Shariff Faleel</button><button class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center undefined">Posts</button><button class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center undefined">Publications</button><button class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center group"><div>Quick links<!-- --><svg fill="currentColor" viewBox="0 0 20 20" class="inline w-4 h-4 mt-1 ml-1 transition-transform duration-200 transform md:-mt-1 rotate-0 group-hover:rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div><div class="absolute w-full mt-2 origin-top-right rounded-md shadow-lg w-fit transition-transform transition-opacity ease-in-out duration-200 opacity-0 scale-0 group-hover:opacity-100 group-hover:scale-100"><div class="p-2 bg-gray-700 rounded-md shadow dark-mode:bg-gray-700"><div class="block px-4 py-2 rounded-lg hover:bg-gray-600 hover:underline hover:decoration-2 items-center justify-left flex flex-row space-x-2" href="https://gist.github.com/ahmed-shariff"> <!-- --><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><title></title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></div><div>github-gists</div></div></div></div></button></div></header><main class="container mx-auto flex-1"><div><div class="grid grid-cols-1 p-0 md:px-20 mt-10"><h1 class="text-xl text-center text-slate-100">Publications</h1><hr class="m-2"/><div class="border border-gray-200 m-2 rounded-xl shadow-md shadow-gray-800 overflow-hidden flex flex-col text-ellipsis h-500 transition duration-100 hover:bg-gray-800"><button class="p-3 text-left"><div class="min-h-full prose dark:prose-invert prose-sm max-w-none prose-h1:text-base prose-h1:font-normal"><h1>HPUI: Hand Proximate User Interfaces for One-Handed Interactions on Head Mounted Displays</h1><div class="text-xs -mt-2"><p><b>Shariff AM Faleel</b>; Michael Gammon; Kevin Fan; Da-Yuan Huang; Wei Li; Pourang Irani</p>
</div><div class="text-xs -mt-2 text-slate-400 flex flex-row space-x-4"><div>August 27, 2021<!-- --></div><div>TVCG</div><a href="https://doi.org/10.1109/TVCG.2021.3106493">10.1109/TVCG.2021.3106493</a></div></div></button></div><div class="border border-gray-200 m-2 rounded-xl shadow-md shadow-gray-800 overflow-hidden flex flex-col text-ellipsis h-500 transition duration-100 hover:bg-gray-800"><button class="p-3 text-left"><div class="min-h-full prose dark:prose-invert prose-sm max-w-none prose-h1:text-base prose-h1:font-normal"><h1>Writely: Force Feedback for Non-Dominant Hand Writing Training</h1><div class="text-xs -mt-2"><p><b>Shariff AM Faleel</b>; Bibhushan Raj Joshi; Bradley Rey</p>
</div><div class="text-xs -mt-2 text-slate-400 flex flex-row space-x-4"><div>July 6, 2021<!-- --></div><div>WHC &#x27;18</div><a href="https://doi.org/10.1109/WHC49131.2021.9517209&#x27;">WHC49131.2021.9517209</a></div></div></button></div><div class="border border-gray-200 m-2 rounded-xl shadow-md shadow-gray-800 overflow-hidden flex flex-col text-ellipsis h-500 transition duration-100 hover:bg-gray-800"><button class="p-3 text-left"><div class="min-h-full prose dark:prose-invert prose-sm max-w-none prose-h1:text-base prose-h1:font-normal"><h1>BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion</h1><div class="text-xs -mt-2"><p>Ali Neshati; Bradley Rey; <b>Ahmed Shariff Mohommed Faleel</b>; Sandra Bardot; Celine Latulipe; Pourang Irani</p>
</div><div class="text-xs -mt-2 text-slate-400 flex flex-row space-x-4"><div>May 7, 2021<!-- --></div><div>CHI &#x27;21</div><a href="https://doi.org/10.1145/3411764.3445201">10.1145/3411764.3445201</a></div></div></button></div><div class="border border-gray-200 m-2 rounded-xl shadow-md shadow-gray-800 overflow-hidden flex flex-col text-ellipsis h-500 transition duration-100 hover:bg-gray-800"><button class="p-3 text-left"><div class="min-h-full prose dark:prose-invert prose-sm max-w-none prose-h1:text-base prose-h1:font-normal"><h1>Using guessability framework: age-related differences in hand gesture interaction</h1><div class="text-xs -mt-2"><p>Yurii Vasylkiv; Ali Neshati; <b>Shariff AM Faleel</b>; Yumiko Sakamoto; Pourang Irani</p>
</div><div class="text-xs -mt-2 text-slate-400 flex flex-row space-x-4"><div>May 27, 2020<!-- --></div><div>Augmented Human (AH &#x27;20)</div><a href="https://doi.org/10.1145/3396339.3396394">10.1145/3396339.3396394</a></div></div></button></div><div class="border border-gray-200 m-2 rounded-xl shadow-md shadow-gray-800 overflow-hidden flex flex-col text-ellipsis h-500 transition duration-100 hover:bg-gray-800"><button class="p-3 text-left"><div class="min-h-full prose dark:prose-invert prose-sm max-w-none prose-h1:text-base prose-h1:font-normal"><h1>User Gesture Elicitation of Common Smartphone Tasks for Hand Proximate User Interfaces</h1><div class="text-xs -mt-2"><p><b>Shariff AM Faleel</b>; Michael Gammon; Yumiko Sakamoto; Carlo Menon; Pourang Irani</p>
</div><div class="text-xs -mt-2 text-slate-400 flex flex-row space-x-4"><div>May 27, 2020<!-- --></div><div>Augmented Human (AH &#x27;20)</div><a href="https://doi.org/10.1145/3396339.3396363">10.1145/3396339.3396363</a></div></div></button></div><div class="border border-gray-200 m-2 rounded-xl shadow-md shadow-gray-800 overflow-hidden flex flex-col text-ellipsis h-500 transition duration-100 hover:bg-gray-800"><button class="p-3 text-left"><div class="min-h-full prose dark:prose-invert prose-sm max-w-none prose-h1:text-base prose-h1:font-normal"><h1>A Novel Dialogue Manager Model for Spoken Dialogue Systems Based on User Input Learning</h1><div class="text-xs -mt-2"><p><b>M.F. Ahmed Shariff</b>; Ruwan D. Nawarathna</p>
</div><div class="text-xs -mt-2 text-slate-400 flex flex-row space-x-4"><div>July 5, 2019<!-- --></div><div>SLAAI-ICAI &#x27;18</div><a href="https://doi.org/10.1007/978-981-13-9129-3_14">10.1007/978-981-13-9129-3_14</a></div></div></button></div></div></div></main><footer class="bg-gray-800 mt-8 py-4 text-gray-300"><div class="container mx-auto flex justify-center text-sm">© 2022 Shariff Faleel</div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"publications":[{"slug":"2021-08-27-shariff21_hpui","frontmatter":{"title":"HPUI: Hand Proximate User Interfaces for One-Handed Interactions on Head Mounted Displays","date":"2021-08-27","authors":"\u003cb\u003eShariff AM Faleel\u003c/b\u003e; Michael Gammon; Kevin Fan; Da-Yuan Huang; Wei Li; Pourang Irani","venue":"TVCG","type":"Journal","paperurl":"https://doi.org/10.1109/TVCG.2021.3106493","doi":"10.1109/TVCG.2021.3106493","citation":"S. A. Faleel, M. Gammon, K. Fan, D. -Y. Huang, W. Li and P. Irani, \"HPUI: Hand Proximate User Interfaces for One-Handed Interactions on Head Mounted Displays,\" in IEEE Transactions on Visualization and Computer Graphics, vol. 27, no. 11, pp. 4215-4225, Nov. 2021, doi: 10.1109/TVCG.2021.3106493.","abstract":"We explore the design of Hand Proximate User Interfaces (HPUIs) for head-mounted displays (HMDs) to facilitate near-body interactions with the display directly projected on, or around the user's hand. We focus on single-handed input, while taking into consideration the hand anatomy which distorts naturally when the user interacts with the display. Through two user studies, we explore the potential for discrete as well as continuous input. For discrete input, HPUIs favor targets that are directly on the fingers (as opposed to off-finger) as they offer tactile feedback. We demonstrate that continuous interaction is also possible, and is as effective on the fingers as in the off-finger space between the index finger and thumb. We also find that with continuous input, content is more easily controlled when the interaction occurs in the vertical or horizontal axes, and less with diagonal movements. We conclude with applications and recommendations for the design of future HPUIs.","pdf":"/pdf/shariff21_hpui.pdf"},"excerpt":""},{"slug":"2021-07-06-shariff21_writely","frontmatter":{"title":"Writely: Force Feedback for Non-Dominant Hand Writing Training","date":"2021-07-06","authors":"\u003cb\u003eShariff AM Faleel\u003c/b\u003e; Bibhushan Raj Joshi; Bradley Rey","venue":"WHC '18","type":"Conference","paperurl":"https://doi.org/10.1109/WHC49131.2021.9517209'","doi":"WHC49131.2021.9517209","citation":"S. A. Faleel, B. Raj Joshi and B. Rey, \"Writely: Force Feedback for Non-Dominant Hand Writing Training,\" 2021 IEEE World Haptics Conference (WHC), 2021, pp. 340-340, doi: 10.1109/WHC49131.2021.9517209.","abstract":"We propose Writely, a haptic force feedback system that uses Haply force feedback device for training non-dominant hand writing. In this work we have developed two different force feedback modalities, Guidance and Anti-Guidance. Through a preliminary exploration, our early results shed light on the potential of Anti-Guidance and a low cost, planar, haptic device specifically for writing motor skill training.","pdf":"/pdf/shariff21_writely.pdf"},"excerpt":""},{"slug":"2021-05-07-ali21_bezelglide","frontmatter":{"title":"BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion","date":"2021-05-07","authors":"Ali Neshati; Bradley Rey; \u003cb\u003eAhmed Shariff Mohommed Faleel\u003c/b\u003e; Sandra Bardot; Celine Latulipe; Pourang Irani","venue":"CHI '21","type":"Conference","paperurl":"https://doi.org/10.1145/3411764.3445201","doi":"10.1145/3411764.3445201","citation":"Ali Neshati, Bradley Rey, Ahmed Shariff Mohommed Faleel, Sandra Bardot, Celine Latulipe, and Pourang Irani. 2021. BezelGlide: Interacting with Graphs on Smartwatches with Minimal Screen Occlusion. In \u003ci\u003eProceedings of the 2021 CHI Conference on Human Factors in Computing Systems\u003c/i\u003e (\u003ci\u003eCHI '21\u003c/i\u003e). Association for Computing Machinery, New York, NY, USA, Article 501, 1–13. https://doi.org/10.1145/3411764.3445201","abstract":"Mid-air gestures have been heavily studied in HCI but with mostly younger adults (YAs). Older adults (OAs) can equally benefit from such a modality, but given their heterogeneous motor abilities, designing suitable gestures is challenging [2]. Our research specifically looks at age-related differences in hand gesture preferences between older and younger adults. This subject is important since it relates to the idea of a proper age-inclusive technological design and the means towards the successful adoption of technologies by all the layers of the population, including older adults.","pdf":"/pdf/ali21_bezelglide.pdf"},"excerpt":""},{"slug":"2020-05-27-yurii20_using_guess","frontmatter":{"title":"Using guessability framework: age-related differences in hand gesture interaction","date":"2020-05-27","authors":"Yurii Vasylkiv; Ali Neshati; \u003cb\u003eShariff AM Faleel\u003c/b\u003e; Yumiko Sakamoto; Pourang Irani","venue":"Augmented Human (AH '20)","type":"Conference","paperurl":"https://doi.org/10.1145/3396339.3396394","doi":"10.1145/3396339.3396394","citation":"Yurii Vasylkiv, Ali Neshati, Shariff A. M. Faleel, Yumiko Sakamoto, and Pourang Irani. 2020. Using guessability framework: age-related differences in hand gesture interaction. In \u003ci\u003eProceedings of the 11th Augmented Human International Conference\u003c/i\u003e (\u003ci\u003eAH '20\u003c/i\u003e). Association for Computing Machinery, New York, NY, USA, Article 24, 1–2. https://doi.org/10.1145/3396339.3396394","abstract":"Mid-air gestures have been heavily studied in HCI but with mostly younger adults (YAs). Older adults (OAs) can equally benefit from such a modality, but given their heterogeneous motor abilities, designing suitable gestures is challenging [2]. Our research specifically looks at age-related differences in hand gesture preferences between older and younger adults. This subject is important since it relates to the idea of a proper age-inclusive technological design and the means towards the successful adoption of technologies by all the layers of the population, including older adults.","pdf":"/pdf/yurii20_using_guess.pdf"},"excerpt":""},{"slug":"2020-05-27-shariff20_user_gesture","frontmatter":{"title":"User Gesture Elicitation of Common Smartphone Tasks for Hand Proximate User Interfaces","date":"2020-05-27","authors":"\u003cb\u003eShariff AM Faleel\u003c/b\u003e; Michael Gammon; Yumiko Sakamoto; Carlo Menon; Pourang Irani","venue":"Augmented Human (AH '20)","type":"Conference","paperurl":"https://doi.org/10.1145/3396339.3396363","doi":"10.1145/3396339.3396363","citation":"Shariff A. M. Faleel, Michael Gammon, Yumiko Sakamoto, Carlo Menon, and Pourang Irani. 2020. User gesture elicitation of common smartphone tasks for hand proximate user interfaces. In \u003ci\u003eProceedings of the 11th Augmented Human International Conference\u003c/i\u003e (\u003ci\u003eAH '20\u003c/i\u003e). Association for Computing Machinery, New York, NY, USA, Article 6, 1–8. https://doi.org/10.1145/3396339.3396363","abstract":"The ubiquity of smartphone interactions along with the advancements made in mixed reality applications and gesture recognition present an intriguing space for novel interaction techniques using the hand as an interface. This paper explores the idea of using hand proximate user interfaces (UI), i.e. interactions with and display of interface elements on and around the hand. We conducted two user studies to gain a better understanding of the design space for such interactions. The first study identifies the possible ways in which various elements can be displayed on and around the hand in the context of common smartphone applications. We conduct a second study to build a gesture set for interactions with elements displayed on and around the hand. We contribute an analysis of the data and observations collected from the two studies, resulting in a layout set and a gesture set for interactions with hand proximate UIs.","pdf":"/pdf/shariff20_user_gesture.pdf"},"excerpt":""},{"slug":"2019-07-05-shariff19_dialog_manager","frontmatter":{"title":"A Novel Dialogue Manager Model for Spoken Dialogue Systems Based on User Input Learning","date":"2019-07-05","authors":"\u003cb\u003eM.F. Ahmed Shariff\u003c/b\u003e; Ruwan D. Nawarathna","venue":"SLAAI-ICAI '18","type":"Conference","paperurl":"https://doi.org/10.1007/978-981-13-9129-3_14","doi":"10.1007/978-981-13-9129-3_14","citation":"Ahmed Shariff, M.F., Nawarathna, R.D. (2019). A Novel Dialogue Manager Model for Spoken Dialogue Systems Based on User Input Learning. In: Hemanth, J., Silva, T., Karunananda, A. (eds) Artificial Intelligence. SLAAI-ICAI 2018. Communications in Computer and Information Science, vol 890. Springer, Singapore. https://doi.org/10.1007/978-981-13-9129-3_14","abstract":"The complexity of the dialogue manager is a major issue in spoken dialogue systems. In this work, a novel dialogue manager based on user input learning is proposed to overcome this issue. In the proposed model back-end functionality is considered as a set of functions a user can trigger through the dialogue manager. It uses these functions as classes for the classification of user inputs. To maintain the context of the dialogue interactions, a context tree is used. Consequently, the model performs its task as two classification tasks to identify the function a user input may trigger and use the context to maintain the discourse of the dialogue. The model shows promising results and proves that a dialogue manager can be integrated into a spoken dialogue system much more directly with less hassle.","pdf":"/pdf/shariff19_dialog_manager.pdf"},"excerpt":""}]},"__N_SSG":true},"page":"/publications","query":{},"buildId":"tfOEPY2JoVhOrpBumd1g3","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>