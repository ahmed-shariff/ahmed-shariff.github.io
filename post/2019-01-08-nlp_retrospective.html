<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/33c9af4c03654ddf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/33c9af4c03654ddf.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-bb4b3df592194c24.js" defer=""></script><script src="/_next/static/chunks/framework-00b57966872fc495.js" defer=""></script><script src="/_next/static/chunks/main-1452d81522d66bc9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-19620cb7a5c3b4e8.js" defer=""></script><script src="/_next/static/chunks/175675d1-a2f4b19cd9daa73f.js" defer=""></script><script src="/_next/static/chunks/987-ab39c8e4b26a30b7.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-1550669730db0549.js" defer=""></script><script src="/_next/static/wnPu1XK5ZTUE55q8lk2KH/_buildManifest.js" defer=""></script><script src="/_next/static/wnPu1XK5ZTUE55q8lk2KH/_ssgManifest.js" defer=""></script><script src="/_next/static/wnPu1XK5ZTUE55q8lk2KH/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen bg-slate-700"><header class="bg-gray-800 mb-0 md:mb-8 py-1 text-gray-300 md:sticky top-0 left-0 right-0 drop-shadow-lg shadow-gray-900 z-10"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous"/><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script><div class="container mx-auto flex flex-col md:flex-row gap-x-12 items-center justify-center"><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center font-semibold text-lg" href="/">Shariff Faleel</a><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center undefined" href="/posts">Posts</a><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center undefined" href="/posts?pub=true">Publications</a><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center group" href="/"><div>Quick links<!-- --><svg fill="currentColor" viewBox="0 0 20 20" class="inline w-4 h-4 mt-1 ml-1 transition-transform duration-200 transform md:-mt-1 rotate-0 group-hover:rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div><div class="absolute w-full mt-2 origin-top-right rounded-md shadow-lg w-fit transition-transform transition-opacity ease-in-out duration-200 opacity-0 scale-0 group-hover:opacity-100 group-hover:scale-100"><div class="p-2 bg-gray-700 rounded-md shadow dark-mode:bg-gray-700"><div class="block px-4 py-2 rounded-lg hover:bg-gray-600 hover:underline hover:decoration-2 items-center justify-left flex flex-row space-x-2" href="https://gist.github.com/ahmed-shariff"> <!-- --><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><title></title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></div><div>github-gists</div></div><div class="block px-4 py-2 rounded-lg hover:bg-gray-600 hover:underline hover:decoration-2 items-center justify-left flex flex-row space-x-2" href="/posts.xml"> <!-- --><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><title></title><path d="M19.199 24C19.199 13.467 10.533 4.8 0 4.8V0c13.165 0 24 10.835 24 24h-4.801zM3.291 17.415c1.814 0 3.293 1.479 3.293 3.295 0 1.813-1.485 3.29-3.301 3.29C1.47 24 0 22.526 0 20.71s1.475-3.294 3.291-3.295zM15.909 24h-4.665c0-6.169-5.075-11.245-11.244-11.245V8.09c8.727 0 15.909 7.184 15.909 15.91z"></path></svg></div><div>RSS feed</div></div></div></div></a></div></header><main class="container mx-auto flex-1"><div class="p-5 md:p-0 prose dark:prose-invert text-justify mx-auto max-w-screen-xl prose-img:block prose-img:m-auto prose-img:max-h-96 prose-p:w-full"><div class="text-xs text-slate-400">January 8, 2019<!-- --><div class="undefined flex gap-x-1 flex-wrap"><a class="text-gray-400" href="/posts?tags=deep+learning">#deep learning</a><a class="text-gray-400" href="/posts?tags=nlp">#nlp</a></div></div><h1 class="text-left">Building a dialogue manager - A retrospective<!-- --></h1><div><p>I presented the paper on the dialogue manager I worked on as part of my undergraduate course at the <a href="https://slaai.lk">SLAII</a>. I had taken a break from the NLP scene and only recently started working on it again. In the spirit of how fast machine learning is progressing, the year-and-a-half-long break has seen alot of progress in this domain as well (I am still figuring out how to keep up with all of it). So, here I am, piecing together my thoughts. I'd be talking about what is it I had done, and why, and then discuss what is being done and the direction I intend to move in.</p>
<h2>A Day in the Dialogue manager's life</h2>
<p>A dialogue manager is part of a spoken dialogue system (SDS). Generally, a spoken dialogue system has five components: Automated speech recognizer (ASR), Natural language understanding (NLU), dialogue manager (DM), Natural language generation (NLG) and Text to speech (TTS). Let me describe what these do through an example dialogue exchange. Say we have a SDS that simply turns on and off a set of lights on a user's behest. Lets say the user starts with saying &quot;hey budd, switch on the light will you&quot;. First the ASR will process the utterance and produce a textual representation of the utterance. The NLU will produce a representation understood by the DM. It can be anything from raw text, POS tags, dialogue act, word embedding, etc. The DM will take the processed utterance and decide what it should do or say. Once it has decided how to respond to what the user said it will send a signal to the NLG which would translate that to a textual representation which in turn will be produced as voice signal by the TTS. That would be the basic cycle the SDS will be taking on every turn. Now to the utterance the user provided: the DM gets the utterance (represented in a way the DM understands) and it understands that the user wants to switch on a light, but the user's input is vague as to which light needs to be switched on. So the DM will need to have to get that piece of information from the user. As such, the NLG gets a signal which it translates to something along the lines of &quot;umm, which light are we talking about?&quot;. While the DM waits for the user to answer the query, it will be keeping tabs on what the user has said in the exchange so far, what has been done in the context of the dialogue exchange, what needs to be done and what the DM is expecting from the user.</p>
<p>Lets consider two scenarios, first the simpler case: the user just says which light to switch on: &quot;light number 3&quot;. Now the DM simply has to send a signal to &quot;light number 3&quot; to switch the light on; end of the dialogue exchange. The second scenario: the user does't know what lights are there or doesn't know how to refer to them, hence  asks: &quot;what lights can you switch on?&quot;. The DM will now put on hold trying to switch on a light and infer what are the lights it can actually work with. Once it has inferred said information, it will relay the information to the user. Lets say the DM decides to say &quot;I can handle the lights one, three, five and six&quot;. Like in scenario one, the user can say &quot;light number three&quot; bring the exchange to a conclusion same as before or say the &quot;second one&quot;. The utterance &quot;second one&quot; on it's own can mean anything, the second world war? the second muffin? Only in the context of the dialogue does this make any sense to the DM. Hence, if I am the DM, this is what I will have to work out:</p>
<ul>
<li>&quot;Second&quot; of what? oh wait, I gave a list in the last turn didn't I! So the user is talking about the &quot;light number three&quot;.</li>
<li>What do I do with &quot;light number three&quot;? Ah, I believe I asked the user which light.</li>
<li>Wait, why did I ask about a light? Right! The user asked to me to turn on a light.</li>
<li>Now I know what to do, &quot;Hey light number three! turn your self on will ya!&quot;</li>
</ul>
<p>How our minds manage to do this seamlessly, that's a whole other discussion. Now that the DM has inferred that the user meant to turn on the &quot;light number three&quot;, the DM can send the signal to &quot;light number three&quot;. To make sure it did get things right, it can ask the user for confirmation on that.</p>
<h2>Ideas behind the model</h2>
<p>There are two core insights I am drawing from to build my model.</p>
<ul>
<li>What an utterance means to the system: an action/operation the system can perform.</li>
<li>Every operation a system can perform can be described using a function.</li>
</ul>
<p>Before I dive into what an utterance means to the system, what does an utterance mean to a human? what is meaning? Well, language on it's own has no meaning, it's simply a collection of symbols and sounds. Meaning arises from what we relate these symbols and sounds to. Which explains why various parts of our brain lights up as we are engaging using language <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. Take for example the question &quot;what is blue&quot;. If a child grows being taught that what we refer to as red is blue, and someday it meets someone out from the world and hear them say that the sky is blue, that child is going to be very confused. Another way of thinking of this is, how do you explain what blue is a to a blind person. In linguistics, among many theories that try to understand meaning and language, the speech act theory has been used widely in the context of dialogue management. The speech act theory states that every utterance made by a human is an action in on itself. The concept of dialogue acts comes from the speech act theory. Broadly speaking, each and every utterance is identified as a dialogue act, such as an information providing act or information requesting act. If we look at the sample scenario discussed earlier, this would be more apparent.</p>
<p>There are a few attempts at standardizing dialogue acts such as DIT++ and DAMSL. The problem with these existing standard dialogue acts is that they are primarily modeled from the perspective of the human, they try to define the dialogue acts to describe what a human would mean. But, what is needed is a set of dialogue acts that the machine can understand. Hence, most of the authors propose more simpler dialogue acts just encapsulate the operations that a system can understand. One particular definition of dialogue acts I found interesting is from <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>. Th author defines three dialogue acts:</p>
<ul>
<li>Information providing act</li>
<li>Information requesting act</li>
<li>Action request act</li>
</ul>
<p>The author identifies that the interaction between a user and a system would only require either both parties exchanging information or asking the other to perform an action, such as switching on the a light. If the previously described dialogue exchange is considered, it can be seen that the all the utterances can be described by one of the three dialogue acts. The user asking the system to switch on the light would be a action request act. The system asking the user which light to switch on or the user asking what lights are available would be information requesting act and the user and system answering these queries would be information providing acts. Each type of act is then mapped to different procedures in the system.</p>
<p>The second insight I draw from is that every action a system can perform can be described by functions or API calls. Consider a GUI interface, for example a web interface to book tickets for a movie. The user would provide the information, and then the system would make an API call or call a function where the information provided by the user would be passed as parameters. The same API call can be made by programmaticaly using a REST interface or a similar interface. Another automated agent also can communicate with this system by issuing a API/function/procedure call. Even the switching on a light can be described as function call, which encapsulates the procedure of sending a signal to the light. Hence, it is not a long stretch to say that, in the context of a discussion about meaning, functions and the values they can take are the primitive entities a machine can understand. All the interactions a machine can have can be described as a sequence of function calls. This idea came about while I was learning (and being amazed by) lisp, which is a whole other story. In lisp, broadly speaking, everything can be defined as a function, from the basic control structures to more complex features like object oriented programming or exception handling. I generalize this perspective to all operations a system can perform. When a user is interacting with the system asking the system to give some information or a do something, using any form of interface, that can be described as the user trying to have the system execute that particular function.</p>
<h2>The model</h2>
<p>The author of <sup class="footnote-ref"><a href="#fn2" id="fnref2:1">[2:1]</a></sup>, in the course of defining the three dialogue acts described before, identifies base elements of a dialogue. The primary elements that are identified were the concerns and replies. Which are then extended to define the acts such that the acts can be connected to procedures relating to them. A concern is simply one party expecting something from the other party in the dialogue exchange. Whereas a reply is one party just providing information, feedback, or performing an action. Taking the perspective that all interactions with a system can be described with functions, a user's concern can be described as the user expecting the system to execute a function and give the results to the user, which would be a system's reply. Similarly, when trying to execute a function, the system may want some piece of information or action from the user, which would be a system's concern. Following that the reply the user gives to the system would be the user's reply. This further reduces the dialogue acts in a task oriented dialogue exchange to two: concerns and replies.</p>
<p>Any task oriented dialogue exchange with the system would start with the system expecting the user's demands, which we will call the root-concern. The root concern is the &quot;how can I help you&quot; in a SDS, or the blinking cursor in a CLI, or a mouse pointer in a GUI. Anything the user asks the system to do would be a reply to this concern. The user saying &quot;hey budd, switch on the light will you&quot; would be a response to this concern. By saying this, the user is expressing/introducing his own concern in this dialogue exchange. To the back-end system, this means the user wants to execute the function which is responsible to turn on a light. Lets assume this function is <em>switch-on-light</em>; ideally this function would take a single parameter, i.e, &quot;which-light&quot;. Hence, the dialogue manager now has to try and execute this function, which is a concern of the DM in on it self. In order for this concern to be resolved, which in turn would resolve the concern the user introduced, the system must execute the function <em>switch-on-light</em>, which cannot be done since it needs a parameter. In order to discern the parameter form the context of the current dialogue exchange the DM would introduce another concern to determine the parameter from the context of the dialogue. From an execution stand-point that would be to execute a parameter extraction function. How a parameter extraction function would work is a topic for another day. For now, this concern is to execute the parameter extraction function which we will call <em>switch-on-light-which-light</em> (since the process of extracting said parameter can be encapsulated using a function). Now when the DM executes this function to resolve the concern last introduced, it will(or at least should) fail as the answer is not present in the context. Looking at the context of the dialogue exchange so far, it can be seen that in order for any of the concerns to be resolved, the concern that represents (the need to execute) <em>switch-on-light-which-light</em> will need to be resolved. This can only be achieved by the system having the user provide this information; which will result in the user being asked for it.</p>
<p>When the user provides this information directly, that is, say &quot;light number 3&quot;, that will (or at least should (*poker face*)) allow the latest concern to be successfully resolved, which will result in the previous concerns being resolved in a cascade. In the case where the user asks for more information on the lights, that would be the user introducing another concern, which from the systems perspective is to execute an function that queries a knowledge base returns the appropriate information. Same as before the system would be introducing a concern to expressing it's desire to execute the said function. Lets assume this function does not take any parameters; which in turn would allow the system to successfully execute the function, effectively resolving the immediate concern and the user's concern to execute the said function. But, it won't resolve the concern pertaining the function <em>switch-on-light-which-light</em> as the parameter expects one value but needs several values. Hence, the user would need to provide the answer either on a prompt from the system or immediately after the systems lists the values. The answer the user provides would be more than sufficient for the <em>switch-on-light-which-light</em> function to discern the value which will resolve it, resulting in the previous concerns being resolved.</p>
<p>Now that the relationship between concerns and replies of both system and user are clear, how does this translate into a model? There are a few significant relationships between these entities that can be observed.</p>
<ol>
<li>All concerns and replies form a rooted tree with the root-concern being the root.</li>
<li>Every system concern encapsulates a function. (Except the root-concern, which is considered a special case)</li>
<li>Every user utterance (concern or reply) is a response to a system concern.</li>
<li>Every user concern tries to trigger a function.</li>
<li>Every system concern (except the root-concern) is a child (or introduced as a result) of a user concern.</li>
</ol>
<p>Following 2 and 3, we can say &quot;each user utterance is a reply to a function&quot;. Similarly, following 4 and 5: &quot;each user concern tries to trigger a function&quot;. As an extension to that: &quot;the difference between a user concern and user reply is if there whether or not it triggers a function&quot;. These (rather ludicrous, but insightful) statements form the backbone of the model. This boils down the core process of the dialogue manager to 2 tasks, i.e. for each user utterance the DM needs to decide the following:</p>
<ol>
<li>Which function is the utterance a reply to?</li>
<li>Which function does the utterance try to trigger? (Where 'none' is a possible answer)</li>
</ol>
<p>These two tasks can be defined using two classifiers, where the prediction of each classifier is one or more functions. The performance of the second task can further divided into two questions (tasks): &quot;Does this utterance tries to trigger a function? if it is trying to do so, what function is it trying to trigger?&quot;. This in-turn can be translated as two classifiers for the second task. The output of the classifiers can be used to decide where in the tree, which we will call the <em>context</em>, does this fit into. After each utterance, the system will go through all the nodes in the context, introduce any concerns it needs to address, try to resolve them if possible. For a concern to be considered as resolved it should not have any unresolved child concerns and it should be have successfully execute the function it is related to (if it is encapsulating a function). This also leads to how a dialogue exchange is considered to be concluding: if the root-concern can be marked as resolved. Once the system has processed the utterance, the state of the tree, or the change in state of the tree as a result of the users utterance is sufficient to decide what the system has to do next: talk about the newly introduced system replies and then talk about the next most important concern the system has. That in conclusion is the model.</p>
<h2>Whats next?</h2>
<p>As with pretty much all facets of AI, dialogue managers and spoken dialogue systems also have been implemented with parts of it or all of it using deep neural networks. There are range of approaches taken, from memory networks to using generative adversarial approaches. These have seen great success. A major limitation I see with these approaches is that they lack transparency. Which in turn means having a trained DL model be adapted to a custom task is questionable, on other hand training a model to the task can be resource intensive and requires a substantial amount of data. I am not saying that DL is bad, personally I believe DNN is a powerful tool. But from experience I can say that there is a huge uncertainty factor when it comes to deep learning which stems from the lack of understanding of how it works. In addition, not being able to properly control the how a dialogue progresses and is processed can be troublesome when trying to integrate it with an application. Another small beef I have with most of the end-to-end approaches is that it attempts to discern the context and meaning though analyzing patterns in the words; in other words, it tries to extract meaning from an entity that has no meaning. As a solution to this, I am working on combining my model with the deep learning approaches. We already know DNN can classify exceptionally well. Also, the surge in adversarial techniques have shown it's potential to synthesizes sentences. Hence a line of inquiry I am working on is to extend my model with these approaches. While it will not be end-to-end differentiable, it's give the advantage of being less opaque. A problem that will require attention in the future is the fact that the approach still requires a alot of data.</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>A 3D Map Of The Brain Shows How We Understand Language: from https://www.popsci.com/3d-map-brain-shows-how-we-find-meaning-through-language accessed: 10th January 2018 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Berg, Markus M., and Bernhard Thalheim. “Modelling of Natural Dialogues in the Context of Speech-Based Information and Control Systems.” @Kiel, Univ., Diss. <a href="#fnref2" class="footnote-backref">↩︎</a> <a href="#fnref2:1" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
</div><div class="m-4 border-t-4 border-slate-400/25"></div><giscus-widget id="comments" repo="ahmed-shariff/ahmed-shariff.github.io" repoid="MDEwOlJlcG9zaXRvcnkxMjU3MDU3Nzc=" category="Announcements" categoryid="DIC_kwDOB34eMc4COpxh" mapping="pathname" reactionsenabled="1" emitmetadata="0" inputposition="top" theme="light" lang="en" loading="lazy"></giscus-widget></div></main><div class="fixed bottom-4 right-4 drop-shadow-lg z-50 shadow-gray-900"><button><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="fill-sky-400 fill-gray-900 rounded-full bg-slate-600 hover:bg-slate-500" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M256 464c114.9 0 208-93.1 208-208S370.9 48 256 48 48 141.1 48 256s93.1 208 208 208zm0-244.5l-81.1 81.9c-7.5 7.5-19.8 7.5-27.3 0s-7.5-19.8 0-27.3l95.7-95.4c7.3-7.3 19.1-7.5 26.6-.6l94.3 94c3.8 3.8 5.7 8.7 5.7 13.7 0 4.9-1.9 9.9-5.6 13.6-7.5 7.5-19.7 7.6-27.3 0l-81-79.9z"></path></svg></button></div><footer class="bg-gray-800 mt-8 py-4 text-gray-300 drop-shadow-lg shadow-gray-900"><div class="container mx-auto flex justify-center text-sm">© 2022 Shariff Faleel</div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"2019-01-08-nlp_retrospective","frontmatter":{"layout":"post","comments":true,"title":"Building a dialogue manager - A retrospective","tags":["deep learning","NLP"],"tagline":"Lesons learnt and thoughts after working on the dialogue management model"},"content":"\nI presented the paper on the dialogue manager I worked on as part of my undergraduate course at the [SLAII](https://slaai.lk). I had taken a break from the NLP scene and only recently started working on it again. In the spirit of how fast machine learning is progressing, the year-and-a-half-long break has seen alot of progress in this domain as well (I am still figuring out how to keep up with all of it). So, here I am, piecing together my thoughts. I'd be talking about what is it I had done, and why, and then discuss what is being done and the direction I intend to move in. \n\n## A Day in the Dialogue manager's life\n\nA dialogue manager is part of a spoken dialogue system (SDS). Generally, a spoken dialogue system has five components: Automated speech recognizer (ASR), Natural language understanding (NLU), dialogue manager (DM), Natural language generation (NLG) and Text to speech (TTS). Let me describe what these do through an example dialogue exchange. Say we have a SDS that simply turns on and off a set of lights on a user's behest. Lets say the user starts with saying \"hey budd, switch on the light will you\". First the ASR will process the utterance and produce a textual representation of the utterance. The NLU will produce a representation understood by the DM. It can be anything from raw text, POS tags, dialogue act, word embedding, etc. The DM will take the processed utterance and decide what it should do or say. Once it has decided how to respond to what the user said it will send a signal to the NLG which would translate that to a textual representation which in turn will be produced as voice signal by the TTS. That would be the basic cycle the SDS will be taking on every turn. Now to the utterance the user provided: the DM gets the utterance (represented in a way the DM understands) and it understands that the user wants to switch on a light, but the user's input is vague as to which light needs to be switched on. So the DM will need to have to get that piece of information from the user. As such, the NLG gets a signal which it translates to something along the lines of \"umm, which light are we talking about?\". While the DM waits for the user to answer the query, it will be keeping tabs on what the user has said in the exchange so far, what has been done in the context of the dialogue exchange, what needs to be done and what the DM is expecting from the user. \n\nLets consider two scenarios, first the simpler case: the user just says which light to switch on: \"light number 3\". Now the DM simply has to send a signal to \"light number 3\" to switch the light on; end of the dialogue exchange. The second scenario: the user does't know what lights are there or doesn't know how to refer to them, hence  asks: \"what lights can you switch on?\". The DM will now put on hold trying to switch on a light and infer what are the lights it can actually work with. Once it has inferred said information, it will relay the information to the user. Lets say the DM decides to say \"I can handle the lights one, three, five and six\". Like in scenario one, the user can say \"light number three\" bring the exchange to a conclusion same as before or say the \"second one\". The utterance \"second one\" on it's own can mean anything, the second world war? the second muffin? Only in the context of the dialogue does this make any sense to the DM. Hence, if I am the DM, this is what I will have to work out:\n\n- \"Second\" of what? oh wait, I gave a list in the last turn didn't I! So the user is talking about the \"light number three\".\n- What do I do with \"light number three\"? Ah, I believe I asked the user which light.\n- Wait, why did I ask about a light? Right! The user asked to me to turn on a light.\n- Now I know what to do, \"Hey light number three! turn your self on will ya!\"\n\nHow our minds manage to do this seamlessly, that's a whole other discussion. Now that the DM has inferred that the user meant to turn on the \"light number three\", the DM can send the signal to \"light number three\". To make sure it did get things right, it can ask the user for confirmation on that.\n\n## Ideas behind the model\n\nThere are two core insights I am drawing from to build my model. \n\n- What an utterance means to the system: an action/operation the system can perform.\n- Every operation a system can perform can be described using a function.\n\nBefore I dive into what an utterance means to the system, what does an utterance mean to a human? what is meaning? Well, language on it's own has no meaning, it's simply a collection of symbols and sounds. Meaning arises from what we relate these symbols and sounds to. Which explains why various parts of our brain lights up as we are engaging using language [^fn-brain-activity]. Take for example the question \"what is blue\". If a child grows being taught that what we refer to as red is blue, and someday it meets someone out from the world and hear them say that the sky is blue, that child is going to be very confused. Another way of thinking of this is, how do you explain what blue is a to a blind person. In linguistics, among many theories that try to understand meaning and language, the speech act theory has been used widely in the context of dialogue management. The speech act theory states that every utterance made by a human is an action in on itself. The concept of dialogue acts comes from the speech act theory. Broadly speaking, each and every utterance is identified as a dialogue act, such as an information providing act or information requesting act. If we look at the sample scenario discussed earlier, this would be more apparent. \n\nThere are a few attempts at standardizing dialogue acts such as DIT++ and DAMSL. The problem with these existing standard dialogue acts is that they are primarily modeled from the perspective of the human, they try to define the dialogue acts to describe what a human would mean. But, what is needed is a set of dialogue acts that the machine can understand. Hence, most of the authors propose more simpler dialogue acts just encapsulate the operations that a system can understand. One particular definition of dialogue acts I found interesting is from [^fn-berg]. Th author defines three dialogue acts: \n\n- Information providing act\n- Information requesting act\n- Action request act\n\nThe author identifies that the interaction between a user and a system would only require either both parties exchanging information or asking the other to perform an action, such as switching on the a light. If the previously described dialogue exchange is considered, it can be seen that the all the utterances can be described by one of the three dialogue acts. The user asking the system to switch on the light would be a action request act. The system asking the user which light to switch on or the user asking what lights are available would be information requesting act and the user and system answering these queries would be information providing acts. Each type of act is then mapped to different procedures in the system.\n\nThe second insight I draw from is that every action a system can perform can be described by functions or API calls. Consider a GUI interface, for example a web interface to book tickets for a movie. The user would provide the information, and then the system would make an API call or call a function where the information provided by the user would be passed as parameters. The same API call can be made by programmaticaly using a REST interface or a similar interface. Another automated agent also can communicate with this system by issuing a API/function/procedure call. Even the switching on a light can be described as function call, which encapsulates the procedure of sending a signal to the light. Hence, it is not a long stretch to say that, in the context of a discussion about meaning, functions and the values they can take are the primitive entities a machine can understand. All the interactions a machine can have can be described as a sequence of function calls. This idea came about while I was learning (and being amazed by) lisp, which is a whole other story. In lisp, broadly speaking, everything can be defined as a function, from the basic control structures to more complex features like object oriented programming or exception handling. I generalize this perspective to all operations a system can perform. When a user is interacting with the system asking the system to give some information or a do something, using any form of interface, that can be described as the user trying to have the system execute that particular function.\n\n## The model\n\nThe author of [^fn-berg], in the course of defining the three dialogue acts described before, identifies base elements of a dialogue. The primary elements that are identified were the concerns and replies. Which are then extended to define the acts such that the acts can be connected to procedures relating to them. A concern is simply one party expecting something from the other party in the dialogue exchange. Whereas a reply is one party just providing information, feedback, or performing an action. Taking the perspective that all interactions with a system can be described with functions, a user's concern can be described as the user expecting the system to execute a function and give the results to the user, which would be a system's reply. Similarly, when trying to execute a function, the system may want some piece of information or action from the user, which would be a system's concern. Following that the reply the user gives to the system would be the user's reply. This further reduces the dialogue acts in a task oriented dialogue exchange to two: concerns and replies. \n\nAny task oriented dialogue exchange with the system would start with the system expecting the user's demands, which we will call the root-concern. The root concern is the \"how can I help you\" in a SDS, or the blinking cursor in a CLI, or a mouse pointer in a GUI. Anything the user asks the system to do would be a reply to this concern. The user saying \"hey budd, switch on the light will you\" would be a response to this concern. By saying this, the user is expressing/introducing his own concern in this dialogue exchange. To the back-end system, this means the user wants to execute the function which is responsible to turn on a light. Lets assume this function is _switch-on-light_; ideally this function would take a single parameter, i.e, \"which-light\". Hence, the dialogue manager now has to try and execute this function, which is a concern of the DM in on it self. In order for this concern to be resolved, which in turn would resolve the concern the user introduced, the system must execute the function _switch-on-light_, which cannot be done since it needs a parameter. In order to discern the parameter form the context of the current dialogue exchange the DM would introduce another concern to determine the parameter from the context of the dialogue. From an execution stand-point that would be to execute a parameter extraction function. How a parameter extraction function would work is a topic for another day. For now, this concern is to execute the parameter extraction function which we will call _switch-on-light-which-light_ (since the process of extracting said parameter can be encapsulated using a function). Now when the DM executes this function to resolve the concern last introduced, it will(or at least should) fail as the answer is not present in the context. Looking at the context of the dialogue exchange so far, it can be seen that in order for any of the concerns to be resolved, the concern that represents (the need to execute) _switch-on-light-which-light_ will need to be resolved. This can only be achieved by the system having the user provide this information; which will result in the user being asked for it.\n\nWhen the user provides this information directly, that is, say \"light number 3\", that will (or at least should (\\*poker face\\*)) allow the latest concern to be successfully resolved, which will result in the previous concerns being resolved in a cascade. In the case where the user asks for more information on the lights, that would be the user introducing another concern, which from the systems perspective is to execute an function that queries a knowledge base returns the appropriate information. Same as before the system would be introducing a concern to expressing it's desire to execute the said function. Lets assume this function does not take any parameters; which in turn would allow the system to successfully execute the function, effectively resolving the immediate concern and the user's concern to execute the said function. But, it won't resolve the concern pertaining the function _switch-on-light-which-light_ as the parameter expects one value but needs several values. Hence, the user would need to provide the answer either on a prompt from the system or immediately after the systems lists the values. The answer the user provides would be more than sufficient for the _switch-on-light-which-light_ function to discern the value which will resolve it, resulting in the previous concerns being resolved. \n\nNow that the relationship between concerns and replies of both system and user are clear, how does this translate into a model? There are a few significant relationships between these entities that can be observed. \n\n1. All concerns and replies form a rooted tree with the root-concern being the root.\n2. Every system concern encapsulates a function. (Except the root-concern, which is considered a special case)\n3. Every user utterance (concern or reply) is a response to a system concern.\n4. Every user concern tries to trigger a function.\n5. Every system concern (except the root-concern) is a child (or introduced as a result) of a user concern.\n\nFollowing 2 and 3, we can say \"each user utterance is a reply to a function\". Similarly, following 4 and 5: \"each user concern tries to trigger a function\". As an extension to that: \"the difference between a user concern and user reply is if there whether or not it triggers a function\". These (rather ludicrous, but insightful) statements form the backbone of the model. This boils down the core process of the dialogue manager to 2 tasks, i.e. for each user utterance the DM needs to decide the following:\n\n1. Which function is the utterance a reply to?\n2. Which function does the utterance try to trigger? (Where 'none' is a possible answer)\n\nThese two tasks can be defined using two classifiers, where the prediction of each classifier is one or more functions. The performance of the second task can further divided into two questions (tasks): \"Does this utterance tries to trigger a function? if it is trying to do so, what function is it trying to trigger?\". This in-turn can be translated as two classifiers for the second task. The output of the classifiers can be used to decide where in the tree, which we will call the _context_, does this fit into. After each utterance, the system will go through all the nodes in the context, introduce any concerns it needs to address, try to resolve them if possible. For a concern to be considered as resolved it should not have any unresolved child concerns and it should be have successfully execute the function it is related to (if it is encapsulating a function). This also leads to how a dialogue exchange is considered to be concluding: if the root-concern can be marked as resolved. Once the system has processed the utterance, the state of the tree, or the change in state of the tree as a result of the users utterance is sufficient to decide what the system has to do next: talk about the newly introduced system replies and then talk about the next most important concern the system has. That in conclusion is the model.\n\n## Whats next?\n\nAs with pretty much all facets of AI, dialogue managers and spoken dialogue systems also have been implemented with parts of it or all of it using deep neural networks. There are range of approaches taken, from memory networks to using generative adversarial approaches. These have seen great success. A major limitation I see with these approaches is that they lack transparency. Which in turn means having a trained DL model be adapted to a custom task is questionable, on other hand training a model to the task can be resource intensive and requires a substantial amount of data. I am not saying that DL is bad, personally I believe DNN is a powerful tool. But from experience I can say that there is a huge uncertainty factor when it comes to deep learning which stems from the lack of understanding of how it works. In addition, not being able to properly control the how a dialogue progresses and is processed can be troublesome when trying to integrate it with an application. Another small beef I have with most of the end-to-end approaches is that it attempts to discern the context and meaning though analyzing patterns in the words; in other words, it tries to extract meaning from an entity that has no meaning. As a solution to this, I am working on combining my model with the deep learning approaches. We already know DNN can classify exceptionally well. Also, the surge in adversarial techniques have shown it's potential to synthesizes sentences. Hence a line of inquiry I am working on is to extend my model with these approaches. While it will not be end-to-end differentiable, it's give the advantage of being less opaque. A problem that will require attention in the future is the fact that the approach still requires a alot of data. \n\n[^fn-berg]: Berg, Markus M., and Bernhard Thalheim. “Modelling of Natural Dialogues in the Context of Speech-Based Information and Control Systems.” @Kiel, Univ., Diss.\n[^fn-brain-activity]: A 3D Map Of The Brain Shows How We Understand Language: from https://www.popsci.com/3d-map-brain-shows-how-we-find-meaning-through-language accessed: 10th January 2018\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2019-01-08-nlp_retrospective"},"buildId":"wnPu1XK5ZTUE55q8lk2KH","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>