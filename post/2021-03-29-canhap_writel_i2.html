<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/e94d3d602c719429.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e94d3d602c719429.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-b84c169ac90f8974.js" defer=""></script><script src="/_next/static/chunks/framework-00b57966872fc495.js" defer=""></script><script src="/_next/static/chunks/main-1452d81522d66bc9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-b0e469cf26f5278f.js" defer=""></script><script src="/_next/static/chunks/175675d1-a2f4b19cd9daa73f.js" defer=""></script><script src="/_next/static/chunks/980-0fae37084f488ec8.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-c00e23fa748dc7e0.js" defer=""></script><script src="/_next/static/1D2-LLEmA_-LiQiTqZWJu/_buildManifest.js" defer=""></script><script src="/_next/static/1D2-LLEmA_-LiQiTqZWJu/_ssgManifest.js" defer=""></script><script src="/_next/static/1D2-LLEmA_-LiQiTqZWJu/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen"><header class="bg-sky-100 mb-8 py-4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous"/><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script><div class="container mx-auto flex justify-center"><a href="/">Shariff Faleel</a></div></header><main class="container mx-auto flex-1"><div class="prose text-justify mx-auto max-w-screen-xl prose-img:block prose-img:m-auto prose-img:max-h-96 prose-p:w-full"><h1>Writely (Part 2) - Face the kraken</h1><div><h3>Recap from <a href="2021-03-08-canhap_writely_i1">iteration 1</a>:</h3>
<p>Our goal was to train the non-dominant hand to write items. In the first iteration we had tried to narrow down what could be done for this project and identify the types of feedback we would want to render to the user, which were:</p>
<ol>
<li>Partial feedback: Where the participants would receive a force feedback when they veer off too much from their path.</li>
<li>Full feedback: Where the user is guided throughout the writing process.</li>
<li>Anti-guiding: In this method, the system actively forces the user off the path the user is expected to follow.</li>
<li>Disturbance: Here the user is provided a random nudge now and then.</li>
</ol>
<p>Consequently we had set out to build a foundational platform and the gather the necessary tools. Primarily, we built a <a href="https://bradleyrrr.github.io/sample/pi1.html#haply-modifications">3D printed pen effector</a> the <a href="2021-03-08-canhap_writely_i1">UI component, tested out a pressure sensor</a> and did some preliminary exploration on <a href="https://joshibibhushan.medium.com/writely-iteration-1-438068380fcc#4077">how to actually &quot;write&quot; using the haply</a>.</p>
<h3>The Kraken</h3>
<p>The goal of this iteration implement and test the different haptic feedback mechanisms. The biggest challenges were around two components. First, was around calculating how &quot;correct&quot; the current position of the user is. If it is just inside-the-letter-or-outside it would be pretty straightforward. But the way we had conceptualized this system, we need to know &quot;how much&quot; inside-the-letter-or-outside we are. The second was about how to render the force. I am going to start by talking about how we tackled the force rendering problem. Then discuss how the positions were calculated and wrap up with how we render the haptic feedbacks.</p>
<h4>Rendering the force</h4>
<p>Initially we thought we'd need the force calculation we had used for the <a href="2021-02-26-canhap-lab4">PID lab</a>. We started by trying to set the values for the P, I and D to do what we wanted. It was only later we had realized that fisca (which we used in <a href="2021-02-05-canhap-lab2">lab 2</a> and then in <a href="2021-03-12-canhap-lab3">lab 3</a>) also basically uses a PID system internally to address the issue of positioning the end effector of the haply. After playing around with the library, to use the positioning functions used by fisca and also add an additional force, we simply had to manipulate the force fisca's physics engine calculates before rending it into haply. This also helped with my lab 3 submission, where I used fisca to render different types of haptic feedback to <a href="2021-03-12-canhap-lab3">convey a &quot;word&quot;</a>. So we wound up completely stripping the PID code and refactoring the system to use fisca.</p>
<h4>Position calculations</h4>
<p>Bibushan had done a pretty cool implementation of using the font files itself to generate the letters in the form of a <code>FPoly</code>. <code>FPloy</code> is one of the classes from fisca, so having the fonts as <code>FPoly</code> objects allows us to connect it with fisca. The following figure demonstrates this system which uses the <code>geommetric</code> library to generate the letters.</p>
<p><img src="/assets/2021-03-29/01_fonts.gif" alt="01"></p>
<p>This method basically uses the outer vertices defined by the font to draw object. All except the disturbance feedback required knowing the center of the letter, i.e, the inner vertices. For the initial iteration, I tried moving horizontally and vertically from the position of the end effector on the screen and picking the closest edges on the screen (in this case, simply check for a change in color). And if I am inside the letter, which we can get using fisca's <code>isTouching</code> method. Following that, the idea was to calculate the closest edge and then get the edge on the opposite vector. The following shows the edge of the font being picked being marked by a red circle.</p>
<p><img src="/assets/2021-03-29/02_vh_border.gif" alt="02"></p>
<p>Two problems stood out which blocked us from proceeding. First, this method is not always reliable, specially when considering letters that have anything other than vertical or horizontal lines. Checking diagonals also could have improved the approximation of the edge. But, the second issue was the showstopper: getting this method to work with letter that have &quot;junctions&quot;, think the letter &quot;T&quot;. Once, we realized it would be tricky to solve this problem, we went back to the drawing board.</p>
<p>Building off Bibushans <code>Alphabets</code> system, I resorted to using the inner vertices for the font, and draw lines with a given thickness to render fonts. Which turned out to be a simpler solution, with a few caveats: First, the inner vertices cannot be drawn from the font files, which means we have to manually calculate and specify the vertices. Second, drawing smooth rounded lines is not possible. But this approaches allowed us to get started on trying out the haptic feedback, so we are using this approach for now. The following image demonstrates the basic idea.</p>
<p><img src="/assets/2021-03-29/03_graph.png" alt="03"></p>
<p>The points provided would be P1, P2 and P3. As shown, using fisca's <code>FLine</code> lines are drawn with a certain thickness. Then to calculate the position the end effector (EE), project the position EE on the extended line of each line segment (blue arrows). Then get the line segment which is closest and also the projection calls inside the line segment. For example, in the above diagram, the EE falls outside the P2 to P3 line segment. Hence will not be used. Hence the problem of guiding or anti-guiding becomes rendering a force proportional to the distance from the EE to the projection on the line segment.</p>
<h3>The haptic feedback</h3>
<p>Through this implementation, we were not able to clearly establish what partial feedback is. Hence for now, we are not considering the partial feedback. The other feedbacks are calculated as follows: For disturbance, Bradely tackled this one and also attempted a mini user study to understand what properties work best. The idea was to randomly generate a force in a random direction.</p>
<p>With the other two, as described in the previous section, we render a force proportional to distance to the closes line segment. We realized having this feedback while outside of the letter (i.e not in touch with the line segment) was not very useful or comfortable. Hence we restricted the force rendering only when the EE is touching any of the <code>FLine</code>s representing the line segments. Another issue we ran into was that when the user first touches the letter with the EE, the force rendered immediately can be too much it completely force the user off the letter. Dampeining didn't help much here. Hence, to avoid this we used a ramping function to ease in the force being rendered. To further smoothen  the force we , we used theFor guidance, it was force proportional to distance vector (blue arrows), and for the anti-guidance it was inversely proportional. The following demos show drawing on a random shape with visualizations of the force being rendered, the closest point picked up on the line, and the drawn letter. (ps: the force lines shown in the below diagram are in the direction of the vector calculated (blue arrrow in above diagram) and not the force itself)</p>
<p>For guiding, I am starting with showing the force being rendered when moving away from the center of the line segment. Then draw around by deliberately trying to go off the path:
<img src="/assets/2021-03-29/04_full_guiding.gif" alt="04"></p>
<p>For anti-guiding:
<img src="/assets/2021-03-29/05_anti_guiding.gif" alt="05"></p>
<h3>Reflections</h3>
<p>As someone used to solving problem with code, it was interesting to see how hardware plays into this. Since the haply's &quot;mood&quot; plays a huge part in this system, relying just on code solution didn't work for us; as a result we had to try different approaches, particularly with calculating the relevant vector values. Naturally, I immensely enjoyed solving the different math and programming problems, figuring out to work with the hardware was much appreciated icing on the cake.</p>
<p>Another interesting observation that came about was with the different haptic feedback. The full guidance seems to take effort off of me, whereas the anti-guidance requires me to pt in more effort into the process to &quot;write&quot; properly. I was not expecting these different approaches to have such a distinct feeling from one another. I still haven't had the chance to watch someone else experience this. Which one would result in a training out motor skills would be the bulk of our third iteration. Bradely had already formulated a questionnaire and a study design, we'll be working off that to further tweak the system to identify which feedback mechanism works best.</p>
<p>Not to mention, we still have a few bugs and technical issues that needs solving (eg. the jittering in the anti-guidance approach). Overall, this has been a pleasant learning experience and I am looking forward to the final iteration of this project.</p>
</div><giscus-widget id="comments" repo="ahmed-shariff/ahmed-shariff.github.io" repoid="MDEwOlJlcG9zaXRvcnkxMjU3MDU3Nzc=" category="Announcements" categoryid="DIC_kwDOB34eMc4COpxh" mapping="pathname" reactionsenabled="1" emitmetadata="0" inputposition="top" theme="light" lang="en" loading="lazy"></giscus-widget></div></main><footer class="bg-sky-100 mt-8 py-4"><div class="container mx-auto flex justify-center text-sm">Â© 2022 Shariff Faleel</div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"layout":"post","comments":true,"title":"Writely (Part 2) - Face the kraken","tags":["course","hci"],"tagline":"Second iteration of the canhap project, where wrestle the haptic implementations on our system."},"content":"\r\n### Recap from [iteration 1]({{ site.baseurl }}{% post_url 2021-03-08-canhap_writely_i1 %}):\r\n\r\nOur goal was to train the non-dominant hand to write items. In the first iteration we had tried to narrow down what could be done for this project and identify the types of feedback we would want to render to the user, which were:\r\n1. Partial feedback: Where the participants would receive a force feedback when they veer off too much from their path.\r\n2. Full feedback: Where the user is guided throughout the writing process.\r\n3. Anti-guiding: In this method, the system actively forces the user off the path the user is expected to follow.\r\n4. Disturbance: Here the user is provided a random nudge now and then.\r\n\r\nConsequently we had set out to build a foundational platform and the gather the necessary tools. Primarily, we built a [3D printed pen effector](https://bradleyrrr.github.io/sample/pi1.html#haply-modifications) the [UI component, tested out a pressure sensor]({{ site.baseurl }}{% post_url 2021-03-08-canhap_writely_i1 %}) and did some preliminary exploration on [how to actually \"write\" using the haply](https://joshibibhushan.medium.com/writely-iteration-1-438068380fcc#4077).\r\n\r\n### The Kraken\r\n\r\nThe goal of this iteration implement and test the different haptic feedback mechanisms. The biggest challenges were around two components. First, was around calculating how \"correct\" the current position of the user is. If it is just inside-the-letter-or-outside it would be pretty straightforward. But the way we had conceptualized this system, we need to know \"how much\" inside-the-letter-or-outside we are. The second was about how to render the force. I am going to start by talking about how we tackled the force rendering problem. Then discuss how the positions were calculated and wrap up with how we render the haptic feedbacks.\r\n\r\n#### Rendering the force\r\n\r\nInitially we thought we'd need the force calculation we had used for the [PID lab]({{ site.baseurl }}{% post_url 2021-02-26-canhap-lab4 %}). We started by trying to set the values for the P, I and D to do what we wanted. It was only later we had realized that fisca (which we used in [lab 2]({{ site.baseurl }}{% post_url 2021-02-05-canhap-lab2 %}) and then in [lab 3]({{ site.baseurl }}{% post_url 2021-03-12-canhap-lab3 %})) also basically uses a PID system internally to address the issue of positioning the end effector of the haply. After playing around with the library, to use the positioning functions used by fisca and also add an additional force, we simply had to manipulate the force fisca's physics engine calculates before rending it into haply. This also helped with my lab 3 submission, where I used fisca to render different types of haptic feedback to [convey a \"word\"]({{ site.baseurl }}{% post_url 2021-03-12-canhap-lab3 %}). So we wound up completely stripping the PID code and refactoring the system to use fisca. \r\n\r\n#### Position calculations\r\n\r\nBibushan had done a pretty cool implementation of using the font files itself to generate the letters in the form of a `FPoly`. `FPloy` is one of the classes from fisca, so having the fonts as `FPoly` objects allows us to connect it with fisca. The following figure demonstrates this system which uses the `geommetric` library to generate the letters.\r\n\r\n![01](/assets/2021-03-29/01_fonts.gif)\r\n\r\nThis method basically uses the outer vertices defined by the font to draw object. All except the disturbance feedback required knowing the center of the letter, i.e, the inner vertices. For the initial iteration, I tried moving horizontally and vertically from the position of the end effector on the screen and picking the closest edges on the screen (in this case, simply check for a change in color). And if I am inside the letter, which we can get using fisca's `isTouching` method. Following that, the idea was to calculate the closest edge and then get the edge on the opposite vector. The following shows the edge of the font being picked being marked by a red circle.\r\n\r\n![02](/assets/2021-03-29/02_vh_border.gif)\r\n\r\nTwo problems stood out which blocked us from proceeding. First, this method is not always reliable, specially when considering letters that have anything other than vertical or horizontal lines. Checking diagonals also could have improved the approximation of the edge. But, the second issue was the showstopper: getting this method to work with letter that have \"junctions\", think the letter \"T\". Once, we realized it would be tricky to solve this problem, we went back to the drawing board. \r\n\r\nBuilding off Bibushans `Alphabets` system, I resorted to using the inner vertices for the font, and draw lines with a given thickness to render fonts. Which turned out to be a simpler solution, with a few caveats: First, the inner vertices cannot be drawn from the font files, which means we have to manually calculate and specify the vertices. Second, drawing smooth rounded lines is not possible. But this approaches allowed us to get started on trying out the haptic feedback, so we are using this approach for now. The following image demonstrates the basic idea.\r\n\r\n![03](/assets/2021-03-29/03_graph.png)\r\n\r\nThe points provided would be P1, P2 and P3. As shown, using fisca's `FLine` lines are drawn with a certain thickness. Then to calculate the position the end effector (EE), project the position EE on the extended line of each line segment (blue arrows). Then get the line segment which is closest and also the projection calls inside the line segment. For example, in the above diagram, the EE falls outside the P2 to P3 line segment. Hence will not be used. Hence the problem of guiding or anti-guiding becomes rendering a force proportional to the distance from the EE to the projection on the line segment.\r\n\r\n### The haptic feedback\r\n\r\nThrough this implementation, we were not able to clearly establish what partial feedback is. Hence for now, we are not considering the partial feedback. The other feedbacks are calculated as follows: For disturbance, Bradely tackled this one and also attempted a mini user study to understand what properties work best. The idea was to randomly generate a force in a random direction.\r\n\r\nWith the other two, as described in the previous section, we render a force proportional to distance to the closes line segment. We realized having this feedback while outside of the letter (i.e not in touch with the line segment) was not very useful or comfortable. Hence we restricted the force rendering only when the EE is touching any of the `FLine`s representing the line segments. Another issue we ran into was that when the user first touches the letter with the EE, the force rendered immediately can be too much it completely force the user off the letter. Dampeining didn't help much here. Hence, to avoid this we used a ramping function to ease in the force being rendered. To further smoothen  the force we , we used theFor guidance, it was force proportional to distance vector (blue arrows), and for the anti-guidance it was inversely proportional. The following demos show drawing on a random shape with visualizations of the force being rendered, the closest point picked up on the line, and the drawn letter. (ps: the force lines shown in the below diagram are in the direction of the vector calculated (blue arrrow in above diagram) and not the force itself)\r\n\r\nFor guiding, I am starting with showing the force being rendered when moving away from the center of the line segment. Then draw around by deliberately trying to go off the path:\r\n![04](/assets/2021-03-29/04_full_guiding.gif)\r\n\r\nFor anti-guiding:\r\n![05](/assets/2021-03-29/05_anti_guiding.gif)\r\n\r\n### Reflections\r\n\r\nAs someone used to solving problem with code, it was interesting to see how hardware plays into this. Since the haply's \"mood\" plays a huge part in this system, relying just on code solution didn't work for us; as a result we had to try different approaches, particularly with calculating the relevant vector values. Naturally, I immensely enjoyed solving the different math and programming problems, figuring out to work with the hardware was much appreciated icing on the cake.\r\n\r\nAnother interesting observation that came about was with the different haptic feedback. The full guidance seems to take effort off of me, whereas the anti-guidance requires me to pt in more effort into the process to \"write\" properly. I was not expecting these different approaches to have such a distinct feeling from one another. I still haven't had the chance to watch someone else experience this. Which one would result in a training out motor skills would be the bulk of our third iteration. Bradely had already formulated a questionnaire and a study design, we'll be working off that to further tweak the system to identify which feedback mechanism works best.\r\n\r\nNot to mention, we still have a few bugs and technical issues that needs solving (eg. the jittering in the anti-guidance approach). Overall, this has been a pleasant learning experience and I am looking forward to the final iteration of this project.\r\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2021-03-29-canhap_writel_i2"},"buildId":"1D2-LLEmA_-LiQiTqZWJu","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>