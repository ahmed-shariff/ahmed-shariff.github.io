<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/3c9597ed95a84348.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3c9597ed95a84348.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-358a2c357f8807e4.js" defer=""></script><script src="/_next/static/chunks/framework-00b57966872fc495.js" defer=""></script><script src="/_next/static/chunks/main-1452d81522d66bc9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-0e8414f5dee81df6.js" defer=""></script><script src="/_next/static/chunks/175675d1-a2f4b19cd9daa73f.js" defer=""></script><script src="/_next/static/chunks/987-ab39c8e4b26a30b7.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-188a99f31c18a6bf.js" defer=""></script><script src="/_next/static/iBqzgouXlCZrswfa2dqdb/_buildManifest.js" defer=""></script><script src="/_next/static/iBqzgouXlCZrswfa2dqdb/_ssgManifest.js" defer=""></script><script src="/_next/static/iBqzgouXlCZrswfa2dqdb/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen bg-slate-700"><header class="bg-gray-800 mb-8 py-1 text-gray-300 md:sticky top-0 left-0 right-0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous"/><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script><div class="container mx-auto flex flex-col md:flex-row gap-x-12 items-center justify-center"><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center font-semibold text-lg" href="/">Shariff Faleel</a><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center undefined" href="/posts">Posts</a><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center undefined" href="/posts?pub=true">Publications</a><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center group" href=""><div>Quick links<!-- --><svg fill="currentColor" viewBox="0 0 20 20" class="inline w-4 h-4 mt-1 ml-1 transition-transform duration-200 transform md:-mt-1 rotate-0 group-hover:rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div><div class="absolute w-full mt-2 origin-top-right rounded-md shadow-lg w-fit transition-transform transition-opacity ease-in-out duration-200 opacity-0 scale-0 group-hover:opacity-100 group-hover:scale-100"><div class="p-2 bg-gray-700 rounded-md shadow dark-mode:bg-gray-700"><div class="block px-4 py-2 rounded-lg hover:bg-gray-600 hover:underline hover:decoration-2 items-center justify-left flex flex-row space-x-2" href="https://gist.github.com/ahmed-shariff"> <!-- --><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><title></title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></div><div>github-gists</div></div></div></div></a></div></header><main class="container mx-auto flex-1"><div class="prose dark:prose-invert text-justify mx-auto max-w-screen-xl prose-img:block prose-img:m-auto prose-img:max-h-96 prose-p:w-full"><div class="text-xs text-slate-400">February 18, 2019<!-- --><div class="undefined flex gap-x-1 flex-wrap"><a class="text-gray-400" href="/posts?tags=machine+learning">#machine learning</a></div></div><h1>The game of labeling</h1><div><p>Given the data hungry models we have, it is no wonder labeling is a significant potion of the process. Working on the restaurant automation project for one and a half year, I constantly had to add more data to the dataset. My workflow primarily consisted of image data, I had data that was collected from different data sources: Web scraping, collected by visiting restaurants, collected in the prototype live environment etc. Me being me, I wound up implementing the labeling pipeline using opencv and python. Initially it was quite simple, with time it started getting more and more complex. I started only needing to add labels, as the project progressed I needed to annotate the data with count, bounding box and segments. The script started getting more and more complex, and we all know how much fun it is to maintain large python scripts. So I took a break and restructured the whole labeling pipeline, which I have iterated upon. At one point, labeling roughly 1000 images with the label and the count for each item in the image took me about 4 hours; which is not fun (forget about the sore fingers, looking at food that long dives me nuts). Now, my current record is, labeling 5136 images with label and counts in g2 hours and 22 minutes, which is about 36 images per minute! Boom!. <strong>Here I outline the process I follow to collect and label data</strong>. I have the metadata framework I use in <a href="2018-07-26-ml-problems">ml problems</a>.</p>
<h3>Collecting Images</h3>
<p>The current setup I have has four cameras that I use to capture the images. I name the images in a manor that the names can be sorted in the order they are captured. The advantage of this is, since we are capturing images from the same scene, the images of the scene are bound to have similar labels. This allows to label these images faster by saying that the image's labels are the same as the image that was labeled before this. In addition to that I've made a habit to follow a pattern when capturing images. For example, if I am starting with capturing images that have rolls and cupcakes, then next I will be capturing images with rolls, cupcakes and pastries. Since the images are named in way to allow sorting in the order they were captured, similar scenes are going to be close by, this also allows to label faster by saying that this scene has labels similar to the scene I labeled before this. The naming convention I follow for the names of the images is as follows:</p>
<pre><code class="hljs">	&lt;<span class="hljs-built_in">time</span>-stamp&gt;<span class="hljs-symbol">_</span>&lt;<span class="hljs-built_in">scene</span>-number&gt;<span class="hljs-symbol">_</span>&lt;camera-number&gt;.jpg
</code></pre>
<p>The time stamp will be the time-stamp of the time the particular image collection session started. And then we have the scene number and camera number. This naming convention, not only allows for the optimizations I mentioned above, but also lets me identify where the image might be coming from. One example of this is when I had to separate the dataset into two based on when it was captured. Due to some mistakes by my hand, the metadata of the files had changed. The fact that I had the time-stamp of the session in which the images were captured saved the day.</p>
<p>Another minor detail that gave me a headache was that if there are no leading zeros to the numbers when they are turned into a string, the sorting by the name would result in order like 199, 20, 21, ..., 200. It is possible to just extract the number from the name and sort based on that (which I implemented just for the kicks of it), but having leading zeros is a lot more convenient. I generally add leading zeros such that the scene numbers are 4 characters and the camera number 2 characters long.</p>
<h3>The labeling pipeline</h3>
<p>Initially with labeling pipeline I had, I would be labeling each image for count, label, crop, and then bounding box. It is a lot less efficient; a better way to do it was to break down the tasks and focus on one task at a time. Each task, handled by a script for each of them, produces an intermediate output which will be used by the next stage. The steps taken are as follows:</p>
<ol>
<li><strong>Class labels</strong>: The script would walk through all the image files pointed to it, and load each file. The user would be prompted to provide the label(s) for each image. The approach I took is have a separate file with all the possible labels I could have and have a number assigned for each of them, and the input provided to the prompt would be these number(s). In addition I allow for a shortcut key to say that the label for the current image is the same as the previous key; which allows to exploit the process used to collect the images to speed up the labeling process. The output of this would be a file which contains the name of the file and the label(s) related to it.</li>
<li><strong>Cleaning labels</strong>: It's very common, atleast for me, to ignore/drop some of the data, due to various reasons like a blurry image. Also, I use this part to assess other discrepancies in the dataset, like rename labels which are misspelled, get the number of images for each class to determine the bias that needs to be addressed in the dataset, etc. The output of this step is a file with the same as above with the irrelevant entries removed and labels and names processed accordingly.</li>
<li><strong>Count/amount/quantity annotations</strong>: In my current project, I need to annotate the dataset with the count/amount/quantity of item(s) in each image. The output of the previous step would have the label and the relevant image file's location, which is used to give the user the appropriate prompts. Since the item(s) in an images are already provided, it will prompt the to enter the count/amount/quantity for each item. Like in the labeling script, here also I have a shortcut set to use the same input used for the previous image. This step will produce an output which contains each image's label(s) and respective count/amount/quantity.</li>
<li>(depending on if I need this)<strong>Bounding annotations</strong>: The next step would be to annotate each image with the bounding box annotations. I still have not figured out a quick way to annotate bounding boxes, it's a tedious task no matter how it's approached. Nonetheless, the output of this step would be, for each image, in addition to the annotations given in the previous steps, the bounding box annotations. Even though it would seem like directly annotating bounding boxes instead of going through the previous steps would be a faster, having that annotation helps to reduce a few extra keystrokes and click, which saved me a significant amount of time.</li>
<li><strong>Process data</strong>: Now that all the relevant data has it's meta data, I generate the data and metadata that will be used to train models. There several reasons I make sure I have the original data with it's metadata untouched and generate the data and metadata separately to be used in the training process. First is that, the original data might be useful for a different task or will require a different pre-processing steps. Having the original data saves alot of pain I have to deal with when something like that happens. Even though it's seems trivial, it's easy to overlook this step. Another reason is that the data that will be used by the training could require a different format, or metadata scheme. I've implemented the data generation scripts to allow to easily modify how the data is generated. In addition, the generation scripts can work with any of the intermediate annotations produced by any of the above steps. Sometime, I only need the labels and count/amount/quantity of the images in a dataset. In that case I would use the output of the step 3 to process the data.
(optional). <strong>Crop</strong>: In some occasions there might be a need to crop images before being used for training. For this, I have a script to add the cropping coordinates. The reason I don't directly crop the data is the same as the reasons I point out in step 5. This cropping coordinates will be stored as another filed in the metadata/annotation files. I generally do this between step 2 and 3, or step 3 and 4. If crop data is provided the step 5 can use this information to crop the output data if it's needed.</li>
</ol>
<p>That would be practices I follow. I will update as I learn and improve the process further. Note that, even though the process outlined is focused on images, it can be adopted to other forms of data.</p>
</div><giscus-widget id="comments" repo="ahmed-shariff/ahmed-shariff.github.io" repoid="MDEwOlJlcG9zaXRvcnkxMjU3MDU3Nzc=" category="Announcements" categoryid="DIC_kwDOB34eMc4COpxh" mapping="pathname" reactionsenabled="1" emitmetadata="0" inputposition="top" theme="light" lang="en" loading="lazy"></giscus-widget></div></main><div class="fixed bottom-4 right-4"><button><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="fill-sky-400 fill-gray-900 rounded-full bg-slate-600" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M256 464c114.9 0 208-93.1 208-208S370.9 48 256 48 48 141.1 48 256s93.1 208 208 208zm0-244.5l-81.1 81.9c-7.5 7.5-19.8 7.5-27.3 0s-7.5-19.8 0-27.3l95.7-95.4c7.3-7.3 19.1-7.5 26.6-.6l94.3 94c3.8 3.8 5.7 8.7 5.7 13.7 0 4.9-1.9 9.9-5.6 13.6-7.5 7.5-19.7 7.6-27.3 0l-81-79.9z"></path></svg></button></div><footer class="bg-gray-800 mt-8 py-4 text-gray-300"><div class="container mx-auto flex justify-center text-sm">© 2022 Shariff Faleel</div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"2019-02-18-labeling_process","frontmatter":{"layout":"post","comments":true,"title":"The game of labeling","tags":["machine learning"],"tagline":"A summery of the labeing process I follow"},"content":"\r\nGiven the data hungry models we have, it is no wonder labeling is a significant potion of the process. Working on the restaurant automation project for one and a half year, I constantly had to add more data to the dataset. My workflow primarily consisted of image data, I had data that was collected from different data sources: Web scraping, collected by visiting restaurants, collected in the prototype live environment etc. Me being me, I wound up implementing the labeling pipeline using opencv and python. Initially it was quite simple, with time it started getting more and more complex. I started only needing to add labels, as the project progressed I needed to annotate the data with count, bounding box and segments. The script started getting more and more complex, and we all know how much fun it is to maintain large python scripts. So I took a break and restructured the whole labeling pipeline, which I have iterated upon. At one point, labeling roughly 1000 images with the label and the count for each item in the image took me about 4 hours; which is not fun (forget about the sore fingers, looking at food that long dives me nuts). Now, my current record is, labeling 5136 images with label and counts in g2 hours and 22 minutes, which is about 36 images per minute! Boom!. **Here I outline the process I follow to collect and label data**. I have the metadata framework I use in [ml problems]({{ site.baseurl }}{% post_url 2018-07-26-ml-problems %}). \r\n\r\n### Collecting Images\r\nThe current setup I have has four cameras that I use to capture the images. I name the images in a manor that the names can be sorted in the order they are captured. The advantage of this is, since we are capturing images from the same scene, the images of the scene are bound to have similar labels. This allows to label these images faster by saying that the image's labels are the same as the image that was labeled before this. In addition to that I've made a habit to follow a pattern when capturing images. For example, if I am starting with capturing images that have rolls and cupcakes, then next I will be capturing images with rolls, cupcakes and pastries. Since the images are named in way to allow sorting in the order they were captured, similar scenes are going to be close by, this also allows to label faster by saying that this scene has labels similar to the scene I labeled before this. The naming convention I follow for the names of the images is as follows:\r\n\r\n```\r\n\t\u003ctime-stamp\u003e_\u003cscene-number\u003e_\u003ccamera-number\u003e.jpg\r\n```\r\n\r\nThe time stamp will be the time-stamp of the time the particular image collection session started. And then we have the scene number and camera number. This naming convention, not only allows for the optimizations I mentioned above, but also lets me identify where the image might be coming from. One example of this is when I had to separate the dataset into two based on when it was captured. Due to some mistakes by my hand, the metadata of the files had changed. The fact that I had the time-stamp of the session in which the images were captured saved the day.\r\n\r\nAnother minor detail that gave me a headache was that if there are no leading zeros to the numbers when they are turned into a string, the sorting by the name would result in order like 199, 20, 21, ..., 200. It is possible to just extract the number from the name and sort based on that (which I implemented just for the kicks of it), but having leading zeros is a lot more convenient. I generally add leading zeros such that the scene numbers are 4 characters and the camera number 2 characters long. \r\n\r\n### The labeling pipeline\r\n\r\nInitially with labeling pipeline I had, I would be labeling each image for count, label, crop, and then bounding box. It is a lot less efficient; a better way to do it was to break down the tasks and focus on one task at a time. Each task, handled by a script for each of them, produces an intermediate output which will be used by the next stage. The steps taken are as follows:\r\n\r\n1. **Class labels**: The script would walk through all the image files pointed to it, and load each file. The user would be prompted to provide the label(s) for each image. The approach I took is have a separate file with all the possible labels I could have and have a number assigned for each of them, and the input provided to the prompt would be these number(s). In addition I allow for a shortcut key to say that the label for the current image is the same as the previous key; which allows to exploit the process used to collect the images to speed up the labeling process. The output of this would be a file which contains the name of the file and the label(s) related to it.\r\n2. **Cleaning labels**: It's very common, atleast for me, to ignore/drop some of the data, due to various reasons like a blurry image. Also, I use this part to assess other discrepancies in the dataset, like rename labels which are misspelled, get the number of images for each class to determine the bias that needs to be addressed in the dataset, etc. The output of this step is a file with the same as above with the irrelevant entries removed and labels and names processed accordingly.\r\n3. **Count/amount/quantity annotations**: In my current project, I need to annotate the dataset with the count/amount/quantity of item(s) in each image. The output of the previous step would have the label and the relevant image file's location, which is used to give the user the appropriate prompts. Since the item(s) in an images are already provided, it will prompt the to enter the count/amount/quantity for each item. Like in the labeling script, here also I have a shortcut set to use the same input used for the previous image. This step will produce an output which contains each image's label(s) and respective count/amount/quantity.\r\n4. (depending on if I need this)**Bounding annotations**: The next step would be to annotate each image with the bounding box annotations. I still have not figured out a quick way to annotate bounding boxes, it's a tedious task no matter how it's approached. Nonetheless, the output of this step would be, for each image, in addition to the annotations given in the previous steps, the bounding box annotations. Even though it would seem like directly annotating bounding boxes instead of going through the previous steps would be a faster, having that annotation helps to reduce a few extra keystrokes and click, which saved me a significant amount of time.\r\n5. **Process data**: Now that all the relevant data has it's meta data, I generate the data and metadata that will be used to train models. There several reasons I make sure I have the original data with it's metadata untouched and generate the data and metadata separately to be used in the training process. First is that, the original data might be useful for a different task or will require a different pre-processing steps. Having the original data saves alot of pain I have to deal with when something like that happens. Even though it's seems trivial, it's easy to overlook this step. Another reason is that the data that will be used by the training could require a different format, or metadata scheme. I've implemented the data generation scripts to allow to easily modify how the data is generated. In addition, the generation scripts can work with any of the intermediate annotations produced by any of the above steps. Sometime, I only need the labels and count/amount/quantity of the images in a dataset. In that case I would use the output of the step 3 to process the data. \r\n(optional). **Crop**: In some occasions there might be a need to crop images before being used for training. For this, I have a script to add the cropping coordinates. The reason I don't directly crop the data is the same as the reasons I point out in step 5. This cropping coordinates will be stored as another filed in the metadata/annotation files. I generally do this between step 2 and 3, or step 3 and 4. If crop data is provided the step 5 can use this information to crop the output data if it's needed.\r\n\r\nThat would be practices I follow. I will update as I learn and improve the process further. Note that, even though the process outlined is focused on images, it can be adopted to other forms of data.\r\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2019-02-18-labeling_process"},"buildId":"iBqzgouXlCZrswfa2dqdb","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>