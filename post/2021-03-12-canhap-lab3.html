<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/e94d3d602c719429.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e94d3d602c719429.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-b84c169ac90f8974.js" defer=""></script><script src="/_next/static/chunks/framework-00b57966872fc495.js" defer=""></script><script src="/_next/static/chunks/main-1452d81522d66bc9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-b0e469cf26f5278f.js" defer=""></script><script src="/_next/static/chunks/175675d1-a2f4b19cd9daa73f.js" defer=""></script><script src="/_next/static/chunks/980-0fae37084f488ec8.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-7e8ee566d594433f.js" defer=""></script><script src="/_next/static/hNyITA6B5TTsWWQzuqx8X/_buildManifest.js" defer=""></script><script src="/_next/static/hNyITA6B5TTsWWQzuqx8X/_ssgManifest.js" defer=""></script><script src="/_next/static/hNyITA6B5TTsWWQzuqx8X/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen"><header class="bg-sky-100 mb-8 py-4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous"/><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script><div class="container mx-auto flex justify-center"><a href="/">Shariff Faleel</a></div></header><main class="container mx-auto flex-1"><div class="prose text-justify mx-auto max-w-screen-xl prose-img:block prose-img:m-auto prose-img:max-h-96 prose-p:w-full"><h1>Talking with haptics</h1><div><h2>For our final lab (or fourth?) in the CanHap course, we were tasked with using haply to communicate affective meaning at a high level. For example, can someone using the haply device made to be felt calm or angry or excited.</h2>
<p>For our final lab (or fourth?) in the CanHap course, we were tasked with using haply to communicate affective meaning at a high level. For example, can someone using the haply device made to be felt calm or angry or excited. Specifically we had to communicate 3 words, I picked <strong>calm, angry, and agitated (excited)</strong>. I picked these three as they fall on different quadrants of the valence-arousal space and are also reasonably separated from each other, which should allow being able to distinguish between them more easily (after all we have to make this work on the haply).</p>
<p><img src="/assets/2021-03-12/Valence-arousal-dimensional-model.png" alt="Valence-arousal-dimensional-model"></p>
<h3>Haplyfying the words</h3>
<h4>Framework</h4>
<p>The previous labs and project work has already given a good idea about what to expect with the haply and what can be done. When thinking of how to convey these words, the analogy that kept coming to my mind was holding your finger in running water or having your hand out in the water while riding a boat (maybe because I can't wait for the Winnipeg winter to be over, ?). Building on this idea what I started implementing was the following:</p>
<ul>
<li>The user holds the actuator which is moved to the center of the screen when the application starts.</li>
<li>When the haptic feedback is rendered, the user feels what is being rendered and describe the word.</li>
</ul>
<p>It didn't take long for me to realize that that might not be something that can be done effectively with the haply. Mainly because the friction and other hardware related parameters require a larger force to applied which makes the meaning I am trying to convey muddled. This was specially tricky as I was trying to render &quot;calm&quot;. After trying a few other things, the approach I settled for is the following:</p>
<ul>
<li>The user moves the end actuator to the center of a circle that is displayed. This circle is within a larger circle.</li>
<li>The user holds the end actuator on the smaller circle inside in a such away it allows the system to control the actuator.</li>
<li>When the user reaches this circle, the haptic feedback will be rendered. The aim of the haptic feedback is to render force(s) that pushes the end-actuator outside the outer circle.</li>
<li>The user can experience the same haptic feedback by moving the end-actuator back to the inner circle.</li>
</ul>
<p>While testing the system, I also realized that the feedback provided is relative. As in, the user will have to have experienced all the feedback before describing it. Which is even more evident when I asked my friend to try it out; which I'll touch on later in this blog.</p>
<h4>The implementation</h4>
<p>The complete implementation is available in <a href="https://github.com/ahmed-shariff/CanHap501_Lab_3">github</a></p>
<p>The main loop is in three stages. Each stage renders the haptic feedback for calm, angry, and agitated, respectively. The &quot;calm&quot; was a tricky one. What I had in mind for it is have a gentle push of the end actuator. I implemented this by gradually ramping up the force rendered until the end-actuator gets pushed out of the outer circle. Since haply doesn't do &quot;gentle&quot; very well I had to try a few different combinations. What worked for me is to have an additional dampening effect to smoothen the ramping force. The other two were a variation of this implementation which can be seen below. For &quot;angry&quot;, instead of ramping up, after a small delay, a large force is rendered instantaneously. For &quot;agitated&quot;, It was the same as &quot;calm&quot; but I added a random noise, which creates a vibration.</p>
<p>Another thing that I had tried at the beginning is to have the direction being rendered to be random, i.e. the direction in which the end actuator gets pushed. Later on I decided against it. The reason was that interpreting haptic feedback is extremely subjective as it is. Reducing the predictability of the feedback makes effective affective communication even more complicated. Hence, I had the direction being rendered to be towards the left side of the screen.</p>
<p>The main loop which renders the haptic feedback can be seen below. (From <a href="https://github.com/ahmed-shariff/CanHap501_Lab_3/blob/29cc9ea5a0e6804120cdc187de4bbfe65f32410e/sketch_words/sketch_words.pde#L143-L172">github (L143 - L172)</a>)</p>
<pre><code class="language-java">levelAngle = 0; //random(-PI, PI);
		
switch (currentLevel){
case 1:// calm
    levelProgress = millis() - levelBase;
    levelForce = 1.5 + levelProgress * 0.0002;
    fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));
    s.h_avatar.setDamping(500);
    break;
case 2:// angry
    levelProgress = millis() - levelBase;

    // delayed force push
    if (levelProgress &gt; 2000)
    {
	levelForce = 3 + levelProgress * 0.0002;
	fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));
    }
    break;
case 3:// agitated
    levelProgress = millis() - levelBase;
    levelForce = 1.5 + levelProgress * 0.001;
    fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));

    // adding noice
    levelAngle = random(-PI, PI);
    fEE.add(1.5 * cos(levelAngle) , 1.5 * sin(levelAngle));
    break;
}
fEE.limit(3.5);
</code></pre>
<p>The following is the video clip of me using the system for the three different words.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/C_CIDn2iOYw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h3>User test</h3>
<p>For the user test, I asked one of my friends to try out my implementation. While he's an experienced HCI researcher and designer, he didn't have much experience with haptics. As described above, I explained what he had to do. Essentially I asked him to &quot;experience&quot; the three different feedback's and then describe what emotion or feeling he'd attach to it at the end. For each word I had here are his responses.</p>
<ul>
<li>For Calm: &quot;Gentle&quot;</li>
<li>For angry: &quot;Rude&quot;</li>
<li>For agitated: &quot;Annoying&quot;</li>
</ul>
<p>I was happy with the results, atleast the responses I got were close to the expected responses with regards to where they fall on the valence-arousal space (the last one was a little of though, nut not suprising).</p>
<h3>Reflection</h3>
<p>It was interesting to see the previous labs as well as the readings we have been doing for the course come together. We had learned how affective communication works as well as how haptics is a subjective user interface. And what are the approaches we can take to make the experience more translatable between different people. Seeing it in action was quite satisfying. I would not have been able to even think of this before this course (I also have become alot more comfortable using the language commonly used by hapticians).</p>
<h4>User test</h4>
<p>My friend made a few interesting comments I think are worth mentioning. When I had described the task to my friend, he thought of the UI as a mouse pointer the idea that the feedback is suppose to push him away from this target sounded unintuitive to him. I explained to think of the system as a notification system as follows: someone is trying to relay an &quot;emotion&quot; or &quot;feeling&quot;, and you received a notification, now you want to know what he relayed, and you are using the system I have explained.</p>
<p>Once he stared using the system, the first haptic feedback rendered was for &quot;calm&quot;. Immediately he says, &quot;this is annoying&quot;. But once He had tried all the different cases, he described the first one as &quot;gentle&quot;. Which emphasizes the fact that haptics is subjective and also tends to be relative. I would assume using the random direction of force feedback I described earlier would have been confusing even more.</p>
<h4>Haply has audio output?</h4>
<p>Then there is actually describing what I have done. This was interesting to me. So far when it comes to setting up my blogs the use of GIF's were sufficient to demonstrate what I had achieved or done. Which was not the case with this lab. I have the original GIF I generated below.</p>
<p><img src="/assets/2021-03-12/01_out.gif" alt="image"></p>
<p>What stood out to me is that it looked like I was moving the actuator instead of the system moving it. Which is not what I wanted to demonstrate. So I had to go the youtube route (I generally prefer all the content be maintained in one place). The only difference between the two is the sound. We were told that audio-visual feedback should not be used, and I didn't. But when I started writing the blog, I am wondering if the audio feedback might have effected the results.</p>
<p>Overall, this was an interesting lab, which makes me appreciate all that I have learned during the course so far.</p>
</div><giscus-widget id="comments" repo="ahmed-shariff/ahmed-shariff.github.io" repoid="MDEwOlJlcG9zaXRvcnkxMjU3MDU3Nzc=" category="Announcements" categoryid="DIC_kwDOB34eMc4COpxh" mapping="pathname" reactionsenabled="1" emitmetadata="0" inputposition="top" theme="light" lang="en" loading="lazy"></giscus-widget></div></main><footer class="bg-sky-100 mt-8 py-4"><div class="container mx-auto flex justify-center text-sm">© 2022 Shariff Faleel</div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"layout":"post","comments":true,"title":"Talking with haptics","tags":["course","hci"],"tagline":"Playing with PID controllers and haply"},"content":"For our final lab (or fourth?) in the CanHap course, we were tasked with using haply to communicate affective meaning at a high level. For example, can someone using the haply device made to be felt calm or angry or excited.\r\n---\r\n\r\nFor our final lab (or fourth?) in the CanHap course, we were tasked with using haply to communicate affective meaning at a high level. For example, can someone using the haply device made to be felt calm or angry or excited. Specifically we had to communicate 3 words, I picked **calm, angry, and agitated (excited)**. I picked these three as they fall on different quadrants of the valence-arousal space and are also reasonably separated from each other, which should allow being able to distinguish between them more easily (after all we have to make this work on the haply).\r\n\r\n![Valence-arousal-dimensional-model](/assets/2021-03-12/Valence-arousal-dimensional-model.png)\r\n\r\n### Haplyfying the words\r\n\r\n#### Framework\r\nThe previous labs and project work has already given a good idea about what to expect with the haply and what can be done. When thinking of how to convey these words, the analogy that kept coming to my mind was holding your finger in running water or having your hand out in the water while riding a boat (maybe because I can't wait for the Winnipeg winter to be over, ?). Building on this idea what I started implementing was the following:\r\n- The user holds the actuator which is moved to the center of the screen when the application starts.\r\n- When the haptic feedback is rendered, the user feels what is being rendered and describe the word.\r\n\r\nIt didn't take long for me to realize that that might not be something that can be done effectively with the haply. Mainly because the friction and other hardware related parameters require a larger force to applied which makes the meaning I am trying to convey muddled. This was specially tricky as I was trying to render \"calm\". After trying a few other things, the approach I settled for is the following:\r\n- The user moves the end actuator to the center of a circle that is displayed. This circle is within a larger circle.\r\n- The user holds the end actuator on the smaller circle inside in a such away it allows the system to control the actuator.\r\n- When the user reaches this circle, the haptic feedback will be rendered. The aim of the haptic feedback is to render force(s) that pushes the end-actuator outside the outer circle. \r\n- The user can experience the same haptic feedback by moving the end-actuator back to the inner circle.\r\n\r\nWhile testing the system, I also realized that the feedback provided is relative. As in, the user will have to have experienced all the feedback before describing it. Which is even more evident when I asked my friend to try it out; which I'll touch on later in this blog. \r\n\r\n#### The implementation\r\n\r\nThe complete implementation is available in [github](https://github.com/ahmed-shariff/CanHap501_Lab_3)\r\n\r\nThe main loop is in three stages. Each stage renders the haptic feedback for calm, angry, and agitated, respectively. The \"calm\" was a tricky one. What I had in mind for it is have a gentle push of the end actuator. I implemented this by gradually ramping up the force rendered until the end-actuator gets pushed out of the outer circle. Since haply doesn't do \"gentle\" very well I had to try a few different combinations. What worked for me is to have an additional dampening effect to smoothen the ramping force. The other two were a variation of this implementation which can be seen below. For \"angry\", instead of ramping up, after a small delay, a large force is rendered instantaneously. For \"agitated\", It was the same as \"calm\" but I added a random noise, which creates a vibration.\r\n\r\nAnother thing that I had tried at the beginning is to have the direction being rendered to be random, i.e. the direction in which the end actuator gets pushed. Later on I decided against it. The reason was that interpreting haptic feedback is extremely subjective as it is. Reducing the predictability of the feedback makes effective affective communication even more complicated. Hence, I had the direction being rendered to be towards the left side of the screen.\r\n\r\nThe main loop which renders the haptic feedback can be seen below. (From [github (L143 - L172)](https://github.com/ahmed-shariff/CanHap501_Lab_3/blob/29cc9ea5a0e6804120cdc187de4bbfe65f32410e/sketch_words/sketch_words.pde#L143-L172))\r\n\r\n```java\r\nlevelAngle = 0; //random(-PI, PI);\r\n\t\t\r\nswitch (currentLevel){\r\ncase 1:// calm\r\n    levelProgress = millis() - levelBase;\r\n    levelForce = 1.5 + levelProgress * 0.0002;\r\n    fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));\r\n    s.h_avatar.setDamping(500);\r\n    break;\r\ncase 2:// angry\r\n    levelProgress = millis() - levelBase;\r\n\r\n    // delayed force push\r\n    if (levelProgress \u003e 2000)\r\n    {\r\n\tlevelForce = 3 + levelProgress * 0.0002;\r\n\tfEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));\r\n    }\r\n    break;\r\ncase 3:// agitated\r\n    levelProgress = millis() - levelBase;\r\n    levelForce = 1.5 + levelProgress * 0.001;\r\n    fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));\r\n\r\n    // adding noice\r\n    levelAngle = random(-PI, PI);\r\n    fEE.add(1.5 * cos(levelAngle) , 1.5 * sin(levelAngle));\r\n    break;\r\n}\r\nfEE.limit(3.5);\r\n```\r\nThe following is the video clip of me using the system for the three different words.\r\n\r\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/C_CIDn2iOYw\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\r\n\r\n### User test\r\n\r\nFor the user test, I asked one of my friends to try out my implementation. While he's an experienced HCI researcher and designer, he didn't have much experience with haptics. As described above, I explained what he had to do. Essentially I asked him to \"experience\" the three different feedback's and then describe what emotion or feeling he'd attach to it at the end. For each word I had here are his responses.\r\n\r\n- For Calm: \"Gentle\"\r\n- For angry: \"Rude\"\r\n- For agitated: \"Annoying\"\r\n\r\nI was happy with the results, atleast the responses I got were close to the expected responses with regards to where they fall on the valence-arousal space (the last one was a little of though, nut not suprising).\r\n\r\n### Reflection\r\n\r\nIt was interesting to see the previous labs as well as the readings we have been doing for the course come together. We had learned how affective communication works as well as how haptics is a subjective user interface. And what are the approaches we can take to make the experience more translatable between different people. Seeing it in action was quite satisfying. I would not have been able to even think of this before this course (I also have become alot more comfortable using the language commonly used by hapticians). \r\n\r\n\r\n#### User test\r\nMy friend made a few interesting comments I think are worth mentioning. When I had described the task to my friend, he thought of the UI as a mouse pointer the idea that the feedback is suppose to push him away from this target sounded unintuitive to him. I explained to think of the system as a notification system as follows: someone is trying to relay an \"emotion\" or \"feeling\", and you received a notification, now you want to know what he relayed, and you are using the system I have explained. \r\n\r\nOnce he stared using the system, the first haptic feedback rendered was for \"calm\". Immediately he says, \"this is annoying\". But once He had tried all the different cases, he described the first one as \"gentle\". Which emphasizes the fact that haptics is subjective and also tends to be relative. I would assume using the random direction of force feedback I described earlier would have been confusing even more.\r\n\r\n#### Haply has audio output?\r\nThen there is actually describing what I have done. This was interesting to me. So far when it comes to setting up my blogs the use of GIF's were sufficient to demonstrate what I had achieved or done. Which was not the case with this lab. I have the original GIF I generated below.\r\n\r\n![image](/assets/2021-03-12/01_out.gif)\r\n\r\nWhat stood out to me is that it looked like I was moving the actuator instead of the system moving it. Which is not what I wanted to demonstrate. So I had to go the youtube route (I generally prefer all the content be maintained in one place). The only difference between the two is the sound. We were told that audio-visual feedback should not be used, and I didn't. But when I started writing the blog, I am wondering if the audio feedback might have effected the results.\r\n\r\nOverall, this was an interesting lab, which makes me appreciate all that I have learned during the course so far.\r\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2021-03-12-canhap-lab3"},"buildId":"hNyITA6B5TTsWWQzuqx8X","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>