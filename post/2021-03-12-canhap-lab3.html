<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/015e7eb62c6848df.css" as="style"/><link rel="stylesheet" href="/_next/static/css/015e7eb62c6848df.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-75c5c7e760a37777.js" defer=""></script><script src="/_next/static/chunks/framework-00b57966872fc495.js" defer=""></script><script src="/_next/static/chunks/main-1452d81522d66bc9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-66833e4259822f9f.js" defer=""></script><script src="/_next/static/chunks/175675d1-a2f4b19cd9daa73f.js" defer=""></script><script src="/_next/static/chunks/987-ab39c8e4b26a30b7.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-9def1390b4c8271b.js" defer=""></script><script src="/_next/static/aw5uZpj0_9_bMhi2KBO2i/_buildManifest.js" defer=""></script><script src="/_next/static/aw5uZpj0_9_bMhi2KBO2i/_ssgManifest.js" defer=""></script><script src="/_next/static/aw5uZpj0_9_bMhi2KBO2i/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen bg-slate-700"><header class="bg-gray-800 mb-0 md:mb-8 py-1 text-gray-300 md:sticky top-0 left-0 right-0 drop-shadow-lg shadow-gray-900"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous"/><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script><div class="container mx-auto flex flex-col md:flex-row gap-x-12 items-center justify-center"><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center font-semibold text-lg" href="/">Shariff Faleel</a><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center undefined" href="/posts">Posts</a><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center undefined" href="/posts?pub=true">Publications</a><a class="transition duration-100 bg-transparent shadow-md shadow-transparent rounded-lg hover:shadow-gray-900 hover:bg-gray-700 hover:underline hover:decoration-2 px-4 py-2 items-center group" href="/"><div>Quick links<!-- --><svg fill="currentColor" viewBox="0 0 20 20" class="inline w-4 h-4 mt-1 ml-1 transition-transform duration-200 transform md:-mt-1 rotate-0 group-hover:rotate-180"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></div><div class="absolute w-full mt-2 origin-top-right rounded-md shadow-lg w-fit transition-transform transition-opacity ease-in-out duration-200 opacity-0 scale-0 group-hover:opacity-100 group-hover:scale-100"><div class="p-2 bg-gray-700 rounded-md shadow dark-mode:bg-gray-700"><div class="block px-4 py-2 rounded-lg hover:bg-gray-600 hover:underline hover:decoration-2 items-center justify-left flex flex-row space-x-2" href="https://gist.github.com/ahmed-shariff"> <!-- --><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><title></title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></div><div>github-gists</div></div><div class="block px-4 py-2 rounded-lg hover:bg-gray-600 hover:underline hover:decoration-2 items-center justify-left flex flex-row space-x-2" href="/posts.xml"> <!-- --><div><svg stroke="currentColor" fill="currentColor" stroke-width="0" role="img" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><title></title><path d="M19.199 24C19.199 13.467 10.533 4.8 0 4.8V0c13.165 0 24 10.835 24 24h-4.801zM3.291 17.415c1.814 0 3.293 1.479 3.293 3.295 0 1.813-1.485 3.29-3.301 3.29C1.47 24 0 22.526 0 20.71s1.475-3.294 3.291-3.295zM15.909 24h-4.665c0-6.169-5.075-11.245-11.244-11.245V8.09c8.727 0 15.909 7.184 15.909 15.91z"></path></svg></div><div>RSS feed</div></div></div></div></a></div></header><main class="container mx-auto flex-1"><div class="p-5 md:p-0 prose dark:prose-invert text-justify mx-auto max-w-screen-xl prose-img:block prose-img:m-auto prose-img:max-h-96 prose-p:w-full"><div class="text-xs text-slate-400">March 12, 2021<!-- --><div class="undefined flex gap-x-1 flex-wrap"><a class="text-gray-400" href="/posts?tags=course">#course</a><a class="text-gray-400" href="/posts?tags=hci">#hci</a></div></div><h1 class="text-left">Talking with haptics</h1><div><h2>For our final lab (or fourth?) in the CanHap course, we were tasked with using haply to communicate affective meaning at a high level. For example, can someone using the haply device made to be felt calm or angry or excited.</h2>
<p>For our final lab (or fourth?) in the CanHap course, we were tasked with using haply to communicate affective meaning at a high level. For example, can someone using the haply device made to be felt calm or angry or excited. Specifically we had to communicate 3 words, I picked <strong>calm, angry, and agitated (excited)</strong>. I picked these three as they fall on different quadrants of the valence-arousal space and are also reasonably separated from each other, which should allow being able to distinguish between them more easily (after all we have to make this work on the haply).</p>
<p><img src="/assets/2021-03-12/Valence-arousal-dimensional-model.png" alt="Valence-arousal-dimensional-model"></p>
<h3>Haplyfying the words</h3>
<h4>Framework</h4>
<p>The previous labs and project work has already given a good idea about what to expect with the haply and what can be done. When thinking of how to convey these words, the analogy that kept coming to my mind was holding your finger in running water or having your hand out in the water while riding a boat (maybe because I can't wait for the Winnipeg winter to be over, ?). Building on this idea what I started implementing was the following:</p>
<ul>
<li>The user holds the actuator which is moved to the center of the screen when the application starts.</li>
<li>When the haptic feedback is rendered, the user feels what is being rendered and describe the word.</li>
</ul>
<p>It didn't take long for me to realize that that might not be something that can be done effectively with the haply. Mainly because the friction and other hardware related parameters require a larger force to applied which makes the meaning I am trying to convey muddled. This was specially tricky as I was trying to render &quot;calm&quot;. After trying a few other things, the approach I settled for is the following:</p>
<ul>
<li>The user moves the end actuator to the center of a circle that is displayed. This circle is within a larger circle.</li>
<li>The user holds the end actuator on the smaller circle inside in a such away it allows the system to control the actuator.</li>
<li>When the user reaches this circle, the haptic feedback will be rendered. The aim of the haptic feedback is to render force(s) that pushes the end-actuator outside the outer circle.</li>
<li>The user can experience the same haptic feedback by moving the end-actuator back to the inner circle.</li>
</ul>
<p>While testing the system, I also realized that the feedback provided is relative. As in, the user will have to have experienced all the feedback before describing it. Which is even more evident when I asked my friend to try it out; which I'll touch on later in this blog.</p>
<h4>The implementation</h4>
<p>The complete implementation is available in <a href="https://github.com/ahmed-shariff/CanHap501_Lab_3">github</a></p>
<p>The main loop is in three stages. Each stage renders the haptic feedback for calm, angry, and agitated, respectively. The &quot;calm&quot; was a tricky one. What I had in mind for it is have a gentle push of the end actuator. I implemented this by gradually ramping up the force rendered until the end-actuator gets pushed out of the outer circle. Since haply doesn't do &quot;gentle&quot; very well I had to try a few different combinations. What worked for me is to have an additional dampening effect to smoothen the ramping force. The other two were a variation of this implementation which can be seen below. For &quot;angry&quot;, instead of ramping up, after a small delay, a large force is rendered instantaneously. For &quot;agitated&quot;, It was the same as &quot;calm&quot; but I added a random noise, which creates a vibration.</p>
<p>Another thing that I had tried at the beginning is to have the direction being rendered to be random, i.e. the direction in which the end actuator gets pushed. Later on I decided against it. The reason was that interpreting haptic feedback is extremely subjective as it is. Reducing the predictability of the feedback makes effective affective communication even more complicated. Hence, I had the direction being rendered to be towards the left side of the screen.</p>
<p>The main loop which renders the haptic feedback can be seen below. (From <a href="https://github.com/ahmed-shariff/CanHap501_Lab_3/blob/29cc9ea5a0e6804120cdc187de4bbfe65f32410e/sketch_words/sketch_words.pde#L143-L172">github (L143 - L172)</a>)</p>
<pre><code class="hljs language-java">levelAngle = <span class="hljs-number">0</span>; <span class="hljs-comment">//random(-PI, PI);</span>
		
<span class="hljs-keyword">switch</span> (currentLevel){
<span class="hljs-keyword">case</span> <span class="hljs-number">1</span>:<span class="hljs-comment">// calm</span>
    levelProgress = millis() - levelBase;
    levelForce = <span class="hljs-number">1.5</span> + levelProgress * <span class="hljs-number">0.0002</span>;
    fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));
    s.h_avatar.setDamping(<span class="hljs-number">500</span>);
    <span class="hljs-keyword">break</span>;
<span class="hljs-keyword">case</span> <span class="hljs-number">2</span>:<span class="hljs-comment">// angry</span>
    levelProgress = millis() - levelBase;

    <span class="hljs-comment">// delayed force push</span>
    <span class="hljs-keyword">if</span> (levelProgress &gt; <span class="hljs-number">2000</span>)
    {
	levelForce = <span class="hljs-number">3</span> + levelProgress * <span class="hljs-number">0.0002</span>;
	fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));
    }
    <span class="hljs-keyword">break</span>;
<span class="hljs-keyword">case</span> <span class="hljs-number">3</span>:<span class="hljs-comment">// agitated</span>
    levelProgress = millis() - levelBase;
    levelForce = <span class="hljs-number">1.5</span> + levelProgress * <span class="hljs-number">0.001</span>;
    fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));

    <span class="hljs-comment">// adding noice</span>
    levelAngle = random(-PI, PI);
    fEE.add(<span class="hljs-number">1.5</span> * cos(levelAngle) , <span class="hljs-number">1.5</span> * sin(levelAngle));
    <span class="hljs-keyword">break</span>;
}
fEE.limit(<span class="hljs-number">3.5</span>);
</code></pre>
<p>The following is the video clip of me using the system for the three different words.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/C_CIDn2iOYw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h3>User test</h3>
<p>For the user test, I asked one of my friends to try out my implementation. While he's an experienced HCI researcher and designer, he didn't have much experience with haptics. As described above, I explained what he had to do. Essentially I asked him to &quot;experience&quot; the three different feedback's and then describe what emotion or feeling he'd attach to it at the end. For each word I had here are his responses.</p>
<ul>
<li>For Calm: &quot;Gentle&quot;</li>
<li>For angry: &quot;Rude&quot;</li>
<li>For agitated: &quot;Annoying&quot;</li>
</ul>
<p>I was happy with the results, atleast the responses I got were close to the expected responses with regards to where they fall on the valence-arousal space (the last one was a little of though, nut not suprising).</p>
<h3>Reflection</h3>
<p>It was interesting to see the previous labs as well as the readings we have been doing for the course come together. We had learned how affective communication works as well as how haptics is a subjective user interface. And what are the approaches we can take to make the experience more translatable between different people. Seeing it in action was quite satisfying. I would not have been able to even think of this before this course (I also have become alot more comfortable using the language commonly used by hapticians).</p>
<h4>User test</h4>
<p>My friend made a few interesting comments I think are worth mentioning. When I had described the task to my friend, he thought of the UI as a mouse pointer the idea that the feedback is suppose to push him away from this target sounded unintuitive to him. I explained to think of the system as a notification system as follows: someone is trying to relay an &quot;emotion&quot; or &quot;feeling&quot;, and you received a notification, now you want to know what he relayed, and you are using the system I have explained.</p>
<p>Once he stared using the system, the first haptic feedback rendered was for &quot;calm&quot;. Immediately he says, &quot;this is annoying&quot;. But once He had tried all the different cases, he described the first one as &quot;gentle&quot;. Which emphasizes the fact that haptics is subjective and also tends to be relative. I would assume using the random direction of force feedback I described earlier would have been confusing even more.</p>
<h4>Haply has audio output?</h4>
<p>Then there is actually describing what I have done. This was interesting to me. So far when it comes to setting up my blogs the use of GIF's were sufficient to demonstrate what I had achieved or done. Which was not the case with this lab. I have the original GIF I generated below.</p>
<p><img src="/assets/2021-03-12/01_out.gif" alt="image"></p>
<p>What stood out to me is that it looked like I was moving the actuator instead of the system moving it. Which is not what I wanted to demonstrate. So I had to go the youtube route (I generally prefer all the content be maintained in one place). The only difference between the two is the sound. We were told that audio-visual feedback should not be used, and I didn't. But when I started writing the blog, I am wondering if the audio feedback might have effected the results.</p>
<p>Overall, this was an interesting lab, which makes me appreciate all that I have learned during the course so far.</p>
</div><div class="m-4 border-t-4 border-slate-400/25"></div><giscus-widget id="comments" repo="ahmed-shariff/ahmed-shariff.github.io" repoid="MDEwOlJlcG9zaXRvcnkxMjU3MDU3Nzc=" category="Announcements" categoryid="DIC_kwDOB34eMc4COpxh" mapping="pathname" reactionsenabled="1" emitmetadata="0" inputposition="top" theme="light" lang="en" loading="lazy"></giscus-widget></div></main><div class="fixed bottom-4 right-4 drop-shadow-lg z-50 shadow-gray-900"><button><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" class="fill-sky-400 fill-gray-900 rounded-full bg-slate-600 hover:bg-slate-500" height="60" width="60" xmlns="http://www.w3.org/2000/svg"><path d="M256 464c114.9 0 208-93.1 208-208S370.9 48 256 48 48 141.1 48 256s93.1 208 208 208zm0-244.5l-81.1 81.9c-7.5 7.5-19.8 7.5-27.3 0s-7.5-19.8 0-27.3l95.7-95.4c7.3-7.3 19.1-7.5 26.6-.6l94.3 94c3.8 3.8 5.7 8.7 5.7 13.7 0 4.9-1.9 9.9-5.6 13.6-7.5 7.5-19.7 7.6-27.3 0l-81-79.9z"></path></svg></button></div><footer class="bg-gray-800 mt-8 py-4 text-gray-300 drop-shadow-lg shadow-gray-900"><div class="container mx-auto flex justify-center text-sm">Â© 2022 Shariff Faleel</div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"slug":"2021-03-12-canhap-lab3","frontmatter":{"layout":"post","comments":true,"title":"Talking with haptics","tags":["course","hci"],"tagline":"Playing with PID controllers and haply"},"content":"For our final lab (or fourth?) in the CanHap course, we were tasked with using haply to communicate affective meaning at a high level. For example, can someone using the haply device made to be felt calm or angry or excited.\r\n---\r\n\r\nFor our final lab (or fourth?) in the CanHap course, we were tasked with using haply to communicate affective meaning at a high level. For example, can someone using the haply device made to be felt calm or angry or excited. Specifically we had to communicate 3 words, I picked **calm, angry, and agitated (excited)**. I picked these three as they fall on different quadrants of the valence-arousal space and are also reasonably separated from each other, which should allow being able to distinguish between them more easily (after all we have to make this work on the haply).\r\n\r\n![Valence-arousal-dimensional-model](/assets/2021-03-12/Valence-arousal-dimensional-model.png)\r\n\r\n### Haplyfying the words\r\n\r\n#### Framework\r\nThe previous labs and project work has already given a good idea about what to expect with the haply and what can be done. When thinking of how to convey these words, the analogy that kept coming to my mind was holding your finger in running water or having your hand out in the water while riding a boat (maybe because I can't wait for the Winnipeg winter to be over, ?). Building on this idea what I started implementing was the following:\r\n- The user holds the actuator which is moved to the center of the screen when the application starts.\r\n- When the haptic feedback is rendered, the user feels what is being rendered and describe the word.\r\n\r\nIt didn't take long for me to realize that that might not be something that can be done effectively with the haply. Mainly because the friction and other hardware related parameters require a larger force to applied which makes the meaning I am trying to convey muddled. This was specially tricky as I was trying to render \"calm\". After trying a few other things, the approach I settled for is the following:\r\n- The user moves the end actuator to the center of a circle that is displayed. This circle is within a larger circle.\r\n- The user holds the end actuator on the smaller circle inside in a such away it allows the system to control the actuator.\r\n- When the user reaches this circle, the haptic feedback will be rendered. The aim of the haptic feedback is to render force(s) that pushes the end-actuator outside the outer circle. \r\n- The user can experience the same haptic feedback by moving the end-actuator back to the inner circle.\r\n\r\nWhile testing the system, I also realized that the feedback provided is relative. As in, the user will have to have experienced all the feedback before describing it. Which is even more evident when I asked my friend to try it out; which I'll touch on later in this blog. \r\n\r\n#### The implementation\r\n\r\nThe complete implementation is available in [github](https://github.com/ahmed-shariff/CanHap501_Lab_3)\r\n\r\nThe main loop is in three stages. Each stage renders the haptic feedback for calm, angry, and agitated, respectively. The \"calm\" was a tricky one. What I had in mind for it is have a gentle push of the end actuator. I implemented this by gradually ramping up the force rendered until the end-actuator gets pushed out of the outer circle. Since haply doesn't do \"gentle\" very well I had to try a few different combinations. What worked for me is to have an additional dampening effect to smoothen the ramping force. The other two were a variation of this implementation which can be seen below. For \"angry\", instead of ramping up, after a small delay, a large force is rendered instantaneously. For \"agitated\", It was the same as \"calm\" but I added a random noise, which creates a vibration.\r\n\r\nAnother thing that I had tried at the beginning is to have the direction being rendered to be random, i.e. the direction in which the end actuator gets pushed. Later on I decided against it. The reason was that interpreting haptic feedback is extremely subjective as it is. Reducing the predictability of the feedback makes effective affective communication even more complicated. Hence, I had the direction being rendered to be towards the left side of the screen.\r\n\r\nThe main loop which renders the haptic feedback can be seen below. (From [github (L143 - L172)](https://github.com/ahmed-shariff/CanHap501_Lab_3/blob/29cc9ea5a0e6804120cdc187de4bbfe65f32410e/sketch_words/sketch_words.pde#L143-L172))\r\n\r\n```java\r\nlevelAngle = 0; //random(-PI, PI);\r\n\t\t\r\nswitch (currentLevel){\r\ncase 1:// calm\r\n    levelProgress = millis() - levelBase;\r\n    levelForce = 1.5 + levelProgress * 0.0002;\r\n    fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));\r\n    s.h_avatar.setDamping(500);\r\n    break;\r\ncase 2:// angry\r\n    levelProgress = millis() - levelBase;\r\n\r\n    // delayed force push\r\n    if (levelProgress \u003e 2000)\r\n    {\r\n\tlevelForce = 3 + levelProgress * 0.0002;\r\n\tfEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));\r\n    }\r\n    break;\r\ncase 3:// agitated\r\n    levelProgress = millis() - levelBase;\r\n    levelForce = 1.5 + levelProgress * 0.001;\r\n    fEE.add(levelForce * cos(levelAngle) , levelForce * sin(levelAngle));\r\n\r\n    // adding noice\r\n    levelAngle = random(-PI, PI);\r\n    fEE.add(1.5 * cos(levelAngle) , 1.5 * sin(levelAngle));\r\n    break;\r\n}\r\nfEE.limit(3.5);\r\n```\r\nThe following is the video clip of me using the system for the three different words.\r\n\r\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/C_CIDn2iOYw\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\r\n\r\n### User test\r\n\r\nFor the user test, I asked one of my friends to try out my implementation. While he's an experienced HCI researcher and designer, he didn't have much experience with haptics. As described above, I explained what he had to do. Essentially I asked him to \"experience\" the three different feedback's and then describe what emotion or feeling he'd attach to it at the end. For each word I had here are his responses.\r\n\r\n- For Calm: \"Gentle\"\r\n- For angry: \"Rude\"\r\n- For agitated: \"Annoying\"\r\n\r\nI was happy with the results, atleast the responses I got were close to the expected responses with regards to where they fall on the valence-arousal space (the last one was a little of though, nut not suprising).\r\n\r\n### Reflection\r\n\r\nIt was interesting to see the previous labs as well as the readings we have been doing for the course come together. We had learned how affective communication works as well as how haptics is a subjective user interface. And what are the approaches we can take to make the experience more translatable between different people. Seeing it in action was quite satisfying. I would not have been able to even think of this before this course (I also have become alot more comfortable using the language commonly used by hapticians). \r\n\r\n\r\n#### User test\r\nMy friend made a few interesting comments I think are worth mentioning. When I had described the task to my friend, he thought of the UI as a mouse pointer the idea that the feedback is suppose to push him away from this target sounded unintuitive to him. I explained to think of the system as a notification system as follows: someone is trying to relay an \"emotion\" or \"feeling\", and you received a notification, now you want to know what he relayed, and you are using the system I have explained. \r\n\r\nOnce he stared using the system, the first haptic feedback rendered was for \"calm\". Immediately he says, \"this is annoying\". But once He had tried all the different cases, he described the first one as \"gentle\". Which emphasizes the fact that haptics is subjective and also tends to be relative. I would assume using the random direction of force feedback I described earlier would have been confusing even more.\r\n\r\n#### Haply has audio output?\r\nThen there is actually describing what I have done. This was interesting to me. So far when it comes to setting up my blogs the use of GIF's were sufficient to demonstrate what I had achieved or done. Which was not the case with this lab. I have the original GIF I generated below.\r\n\r\n![image](/assets/2021-03-12/01_out.gif)\r\n\r\nWhat stood out to me is that it looked like I was moving the actuator instead of the system moving it. Which is not what I wanted to demonstrate. So I had to go the youtube route (I generally prefer all the content be maintained in one place). The only difference between the two is the sound. We were told that audio-visual feedback should not be used, and I didn't. But when I started writing the blog, I am wondering if the audio feedback might have effected the results.\r\n\r\nOverall, this was an interesting lab, which makes me appreciate all that I have learned during the course so far.\r\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2021-03-12-canhap-lab3"},"buildId":"aw5uZpj0_9_bMhi2KBO2i","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>