<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/e94d3d602c719429.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e94d3d602c719429.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-00b57966872fc495.js" defer=""></script><script src="/_next/static/chunks/main-1452d81522d66bc9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-b0e469cf26f5278f.js" defer=""></script><script src="/_next/static/chunks/175675d1-a2f4b19cd9daa73f.js" defer=""></script><script src="/_next/static/chunks/980-0fae37084f488ec8.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-7cde54ec86176475.js" defer=""></script><script src="/_next/static/RaWL0xzu475sacfgiyh76/_buildManifest.js" defer=""></script><script src="/_next/static/RaWL0xzu475sacfgiyh76/_ssgManifest.js" defer=""></script><script src="/_next/static/RaWL0xzu475sacfgiyh76/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen"><header class="bg-sky-100 mb-8 py-4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous"/><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script><div class="container mx-auto flex justify-center"><a href="/">Shariff Faleel</a></div></header><main class="container mx-auto flex-1"><div class="prose text-justify mx-auto max-w-screen-xl prose-img:block prose-img:m-auto prose-img:max-h-96 prose-p:w-full"><h1>ML problems I run into</h1><div><p>This is an ongoing list of problems I keep running into when working on ML models. Some of them I address by adding a feature to my <a href="https://github.com/ahmed-shariff/ml-pipeline">machine learning pipeline</a>, others I note down as a 'best practice'</p>
<h3>Varying dataset classes count</h3>
<p>While a relatively smaller and uncommon problem, I happen to run into this problem now and then. On occasions I only need to use a subset of the classes in a dataset. For example in a dataset with 100 classes, I  might use only 20 classes for a classification task. In this case, ideally, the number of outputs of the model output would be 20. But the annotations of the dataset is going to have the class labels in the range of 0 - 99. If the output has only 20 nodes, unless the classes are labeled 0-19, I'd run into <code>index  out of bounds</code> problems as the training loop will try to assess loss for labels beyond 20.</p>
<p>One solution is to remap the classes I am using to the range of the output of the model. But the problem with this is that I have to separately maintain the remapping to refer to the class an output node represents. The second solution is to make the number of output nodes to equal to the total number of classes in the dataset. While this adds a small overhead to the model, I find it easier to maintain the models.</p>
<p>It should be noted that the this is a manifestation of the strong relationship between data and code in machine learning.</p>
<h3>Model code and weight</h3>
<p>When I train a model, the model's graph itself is not stored. Only the weights are stored. Hence if the code is changed in any way, the trained weights become useless. I can think of two ways to solve this problem:</p>
<ul>
<li>Save the code with the weights.</li>
<li>Use version control (git) and save a reference to the version along with the weights, and record which experiment the saved weights belong to.</li>
</ul>
<p>Both feel like not so good approaches: If I want to reproduce results either save multiple copies of code or have to work through the version control tool while maintaining a separate log to know which saved files belong to which experiment. I use  <a href="2018-06-11-Experiment-log">experiment logs</a> in which I record the versions (commits hash, as I use git), which kinda eases the pain of maintaining experiment-code-weight-data combo. But I would like to have better control over things. For now I record the commit hash in the experiment log and save a copy of the code with the saved weights. In the future I hope to have the pipeline log the commit hash when it's saving weights. As an extension to that, perhaps I can run a diff between the current file and the file from the commit to assess of the code has changed, and use that output.</p>
<h3>Meta data models</h3>
<p>Each dataset comes with it's own metadata model. This tends to get annoying when you have to build a data-loader separately for each dataset. A solution to this problem is to adopt a meta-data model. An interesting approach in this regard that I have come across is the <em>feature store</em> in Uber's <em>Michelangelo</em>. For now I am following a json format for image data. Where each dataset is represented as a json array. Each image is represented as a array, with the first element pointing to the name of the image. The remaining elements are structured based on what they represent:</p>
<ul>
<li>class labels: a string, if images contain multiple classes, an array of strings.
<ul>
<li>eg: [car, bike, person]</li>
</ul>
</li>
<li>counts: similar to the above, but instead of strings, an object with one attribute - the name is a class label, the value is the count of that item.
<ul>
<li>eg: [{car:2}, {bike:3}, {person: 10}]</li>
</ul>
</li>
<li>Bounding boxes: a list of json objects. Each containing 5 attributes - class labels and the coordinates.
<ul>
<li>eg: [{name: car, x1:10, x2: 250, y1:22, y2: 320}, {name: bike, x1:42, x2: 33, y1:67, y2: 53}]</li>
</ul>
</li>
<li>Segmentation mask: a string pointing to the file containing the mask for the respective image.</li>
</ul>
<p>Generally, instead of having the class labels as strings, it's ideal to have them as integers. In such circumstances, I maintain a separate file for the mapping from class label to a unique integer.</p>
<p>Something like the pascal VOC format is another choice, but I am not a fan of having separate files for each image. Specially because the data annotation pipeline I have in place sometimes goes through several phases which can require multiple files for each image, which can become cumbersome to maintain.</p>
<h3>Remembering why.....</h3>
<p>Training deep learning models takes time. It can be a few hours, or even days. While the experiment log helps keep things a little tidy, it can be hard to keep track of your thought process. Instead of sitting around and waiting for a model to be trained I like to switch to a different task and work on that, perhaps setting up another experiment. With each model queued, it's very normal to make quite a few adjustments. Even with extensive logs and meticulous version controlling not having my thought process as to why I made the changes I made can be very problematic. While I can capture these in the experiment log itself, I find it alot more convenient to have such entries in a separate document and have only the more refined and complete ideas in the experiment log. I maintain another org file which I use more like a journal. Each entry is recorded under a date, and appended with a link to the log in the experiment log it is related to. I've bound this to an org-capture template, which makes it a lot easier to make these entries.</p>
</div></div></main><footer class="bg-sky-100 mt-8 py-4"><div class="container mx-auto flex justify-center text-sm">Â© 2022 Shariff Faleel</div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"layout":"post","comments":true,"title":"ML problems I run into","tags":["Random thoughts"],"tagline":"An ongoing list of problems I run into when using deep learning"},"content":"\r\nThis is an ongoing list of problems I keep running into when working on ML models. Some of them I address by adding a feature to my [machine learning pipeline](https://github.com/ahmed-shariff/ml-pipeline), others I note down as a 'best practice'\r\n\r\n### Varying dataset classes count\r\nWhile a relatively smaller and uncommon problem, I happen to run into this problem now and then. On occasions I only need to use a subset of the classes in a dataset. For example in a dataset with 100 classes, I  might use only 20 classes for a classification task. In this case, ideally, the number of outputs of the model output would be 20. But the annotations of the dataset is going to have the class labels in the range of 0 - 99. If the output has only 20 nodes, unless the classes are labeled 0-19, I'd run into \u003ccode\u003eindex  out of bounds\u003c/code\u003e problems as the training loop will try to assess loss for labels beyond 20.\r\n\r\nOne solution is to remap the classes I am using to the range of the output of the model. But the problem with this is that I have to separately maintain the remapping to refer to the class an output node represents. The second solution is to make the number of output nodes to equal to the total number of classes in the dataset. While this adds a small overhead to the model, I find it easier to maintain the models.\r\n\r\nIt should be noted that the this is a manifestation of the strong relationship between data and code in machine learning.\r\n\r\n### Model code and weight \r\nWhen I train a model, the model's graph itself is not stored. Only the weights are stored. Hence if the code is changed in any way, the trained weights become useless. I can think of two ways to solve this problem:\r\n\r\n* Save the code with the weights.\r\n* Use version control (git) and save a reference to the version along with the weights, and record which experiment the saved weights belong to.\r\n\r\nBoth feel like not so good approaches: If I want to reproduce results either save multiple copies of code or have to work through the version control tool while maintaining a separate log to know which saved files belong to which experiment. I use  [experiment logs]({{ site.baseurl }}{% post_url 2018-06-11-Experiment-log %}) in which I record the versions (commits hash, as I use git), which kinda eases the pain of maintaining experiment-code-weight-data combo. But I would like to have better control over things. For now I record the commit hash in the experiment log and save a copy of the code with the saved weights. In the future I hope to have the pipeline log the commit hash when it's saving weights. As an extension to that, perhaps I can run a diff between the current file and the file from the commit to assess of the code has changed, and use that output.\r\n\r\n### Meta data models\r\nEach dataset comes with it's own metadata model. This tends to get annoying when you have to build a data-loader separately for each dataset. A solution to this problem is to adopt a meta-data model. An interesting approach in this regard that I have come across is the *feature store* in Uber's *Michelangelo*. For now I am following a json format for image data. Where each dataset is represented as a json array. Each image is represented as a array, with the first element pointing to the name of the image. The remaining elements are structured based on what they represent:\r\n- class labels: a string, if images contain multiple classes, an array of strings.\r\n  - eg: [car, bike, person]\r\n- counts: similar to the above, but instead of strings, an object with one attribute - the name is a class label, the value is the count of that item.\r\n  - eg: [{car:2}, {bike:3}, {person: 10}]\r\n- Bounding boxes: a list of json objects. Each containing 5 attributes - class labels and the coordinates.\r\n  - eg: [{name: car, x1:10, x2: 250, y1:22, y2: 320}, {name: bike, x1:42, x2: 33, y1:67, y2: 53}]\r\n- Segmentation mask: a string pointing to the file containing the mask for the respective image.\r\n\r\nGenerally, instead of having the class labels as strings, it's ideal to have them as integers. In such circumstances, I maintain a separate file for the mapping from class label to a unique integer.\r\n\r\nSomething like the pascal VOC format is another choice, but I am not a fan of having separate files for each image. Specially because the data annotation pipeline I have in place sometimes goes through several phases which can require multiple files for each image, which can become cumbersome to maintain.\r\n\r\n### Remembering why.....\r\nTraining deep learning models takes time. It can be a few hours, or even days. While the experiment log helps keep things a little tidy, it can be hard to keep track of your thought process. Instead of sitting around and waiting for a model to be trained I like to switch to a different task and work on that, perhaps setting up another experiment. With each model queued, it's very normal to make quite a few adjustments. Even with extensive logs and meticulous version controlling not having my thought process as to why I made the changes I made can be very problematic. While I can capture these in the experiment log itself, I find it alot more convenient to have such entries in a separate document and have only the more refined and complete ideas in the experiment log. I maintain another org file which I use more like a journal. Each entry is recorded under a date, and appended with a link to the log in the experiment log it is related to. I've bound this to an org-capture template, which makes it a lot easier to make these entries.\r\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2018-07-26-ml-problems"},"buildId":"RaWL0xzu475sacfgiyh76","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>