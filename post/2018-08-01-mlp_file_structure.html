<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="/_next/static/css/eedc2b9f1f7eca87.css" as="style"/><link rel="stylesheet" href="/_next/static/css/eedc2b9f1f7eca87.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-be33543a527a1d95.js" defer=""></script><script src="/_next/static/chunks/framework-00b57966872fc495.js" defer=""></script><script src="/_next/static/chunks/main-1452d81522d66bc9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-8ca82eb2b0d3d831.js" defer=""></script><script src="/_next/static/chunks/175675d1-a2f4b19cd9daa73f.js" defer=""></script><script src="/_next/static/chunks/980-0fae37084f488ec8.js" defer=""></script><script src="/_next/static/chunks/pages/post/%5Bslug%5D-8fbd1c4524acd0de.js" defer=""></script><script src="/_next/static/o1NKsy16OVz-k8e3hzwe4/_buildManifest.js" defer=""></script><script src="/_next/static/o1NKsy16OVz-k8e3hzwe4/_ssgManifest.js" defer=""></script><script src="/_next/static/o1NKsy16OVz-k8e3hzwe4/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div class="flex flex-col min-h-screen bg-slate-700"><header class="bg-gray-800 mb-8 py-4 text-gray-300"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css" integrity="sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB" crossorigin="anonymous"/><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js" integrity="sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p" crossorigin="anonymous"></script><script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script><div class="container mx-auto flex flex-row space-x-12 justify-center items-center"><div class="font-semibold text-xl"><a href="/">Shariff Faleel</a></div><a href="/posts">posts</a></div></header><main class="container mx-auto flex-1"><div class="prose dark:prose-invert text-justify mx-auto max-w-screen-xl prose-img:block prose-img:m-auto prose-img:max-h-96 prose-p:w-full"><h1>Practices I follow with the machine learning pipeline</h1><div><p><em><small>Edit [01-01-2019]: Changed the reference to the old mlpipeline to the new</small></em></p>
<p>While the <a href="https://github.com/ahmed-shariff/ml-pipeline">ml-pipeline</a> and [experiment log]({{ site.baseurl }}{% post_url 2018-06-11-Experiment-log%}) I use gives me a better control over my machine learning projects, they don't cover all the bases. Here I outline some of the practices I have grown to follow to make my life easier. Note that by no means is this a perfect process. I am simply documenting my current practices here, which I hope can help me as well as others, in the future. I also am listing down some of the problems when working on machine learning in [ML problems I run into]({{ site.baseurl }}{% post_url 2018-07-26-ml-problems%}).</p>
<p>I generally structure my machine learning project into four directories:</p>
<ul>
<li>Datasets: A directory that contains the data and related materials (meta-data, configuration files, data related scripts, etc.)</li>
<li>Models: A directory that contains the code for the models and related tools. This directory will also contain the output files of the ml-pipeline.</li>
<li>Deployment: If the models I am working on is expected to go into a production environment, any code related to testing trained models in such context goes in here.</li>
<li>Data_processing: Sometimes I can't use the data made available straight away. I use tend to write scripts to clean the data, which I store here.</li>
</ul>
<p>In addition the projects root will contain the configuration files used by the ml-pipeline.</p>
<p>The function of the directories are divided such that, when training, only the <code>Datasets</code> and <code>Models</code> directories are used. When working on piecing together the trained model for deployment, only the <code>Datasets</code> and <code>Deployment</code> directories are used. Likewise, when I am working on the dataset, only the <code>Datasets</code> and <code>Data_processing</code> directories are used.</p>
<p>The above division of the workflow can be thought of as a lose <em>separation of concerns</em>. In machine learning defining clear boundaries for components, which then can be improved and tested in isolation like in a traditional software system, is very tricky. While this division of the workflow is not a perfect solution, it works well for me, <em>for now</em>. In the remainder of this article I will try to describe how I define and maintain the content of each of this directories (components).</p>
<h2>Datasets</h2>
<p>This directory primarily contains the datasets and related content. That is, image files, video files, audio files, stored embedding, etc. This also contains the meta data of the datasets. Occasionally, if there are any tools/scripts, which are used to make necessary changes to the datasets, are also stored here. This allows for the use of a tool like [dvc][https://github.com/iterative/dvc] to maintain the data. In addition I also store any pre-trained weights in this directory. The pre-trained weights are stored here as it can be used in both the training process and deployment systems. As each dataset tends to have it's own annotation/meta data format, I prefer to convert the meta-data to a fixed [format]({{ site.baseurl }}{% post_url 2018-07-26-ml-problems%}#meta-data-models) which eases the work that needs to be done when defining data-loaders. As of now I am using a json format. In the future I'll be moving to a better meta-data model, that pulls from a properly maintained database of features<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. The json format I use for now is as follows:</p>
<ul>
<li>class labels: a string, if images contain multiple classes, an array of strings.
<ul>
<li>eg:
<ul>
<li>Single item: <code>car</code></li>
<li>Multiple items:<code>[car, bike, person]</code></li>
</ul>
</li>
</ul>
</li>
<li>counts: similar to the above, but instead of strings, an object with one attribute - the name is a class label, the value is the count of that item.
<ul>
<li>eg:
<ul>
<li>Single item: <code>{car:2}</code></li>
<li>Multiple items: <code>[{car:2}, {bike:3}, {person: 10}]</code></li>
</ul>
</li>
</ul>
</li>
<li>Bounding boxes: a list of json objects (pretty much always there is more than one bounding box). Each containing 5 attributes - class labels and the coordinates.
<ul>
<li>eg: <code>[{name: car, x1:10, x2: 250, y1:22, y2: 320},
{name: bike, x1:42, x2: 33, y1:67, y2: 53}]</code></li>
</ul>
</li>
<li>Segmentation mask: a string pointing to the file containing the mask for the respective image.</li>
</ul>
<h2>Models</h2>
<p>Here I host the models and related content and this is where the core of my work takes place. For each model I am developing I have a separate script. To keep things cleaner, I prepend the id of the primary experiment the model in the script is related to. Also, the pipeline stores the outputs in this directory. This way this directory can be maintained as a separate repository using version control to keep track of the experiments conducted. I have made the habit of making a commit before and after a training session and record the commit's hash in the experiment_log <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<p>Some parts of the code in several models scripts can be shared. The code in the script can be divided into two segments:</p>
<ul>
<li>Code related to the construction of the model.</li>
<li>Code related to the training and evaluation process of the model.</li>
</ul>
<p>In most cases it is the second category of code that is shared among the scripts. When I export a model, I [make a copy of the script]({{ site.baseurl }}{% post_url 2018-07-26-ml-problems%}#model-code-and-weight) and store it along with the weights. For an exported model, the training and evaluation code is mostly irrelevant. Hence, only the code that falls into the second category is abstracted to a separate script which can be imported here. Any code that is related to the construction of a model will be available the respective script. As such, there are several components that I have delegated to a different script, such as the data-loaders and implantation details specific to a library (saving and loading weights, setting hyper-parameters, cleaning saved cache of previous iterations, etc.). Note that if using an intermediate format which exports both the weights and the graph (ONNX or tensorflow's <em>saved models</em>), this will not be issue <sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>.</p>
<h2>Deployment</h2>
<p>Anything beyond training, this is the place where I have em all. Once I have a model I am comfortable with, I export it and build the necessary pieces needed to deploy/test the exported model. Generally I use the script I have saved along with the exported model to construct the model and set up other components around it.</p>
<h2>Data processing</h2>
<p>Since 'machine learning', data is always a vital part. Almost always, all projects have new data. Even though deep learning might alleviate the need for feature engineering, the data generally needs to be cleaned and processed before being used for training. While I prefer storing the data itself in the <code>Datasets</code> directory, the scripts I use to process the data and the related intermediate states and meta-data, I store in this directory. This is mostly to avoid polluting the <code>Datasets</code> directory. This way, the dirty stuff is in here, while the <code>Datasets</code> directory will be holding the actual data and meta-data in it.</p>
<h2>Content in the root directory</h2>
<p>The root directory primarily would contain the configuration files used by the pipeline and the experiment log and the [notes]({{ site.baseurl }}{% post_url 2018-07-26-ml-problems%}#).</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>The coco annotation format is a solution I am looking at for a propper meta-data model. <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Hoping to make commiting and recording before and after each experiment a feature in the ml-pipeline. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>An alternate approach I am working on is to export all files loaded/imported from the <em>models</em> directory. That is, a copy of the scripts used will be saved along with the model. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
</div><giscus-widget id="comments" repo="ahmed-shariff/ahmed-shariff.github.io" repoid="MDEwOlJlcG9zaXRvcnkxMjU3MDU3Nzc=" category="Announcements" categoryid="DIC_kwDOB34eMc4COpxh" mapping="pathname" reactionsenabled="1" emitmetadata="0" inputposition="top" theme="light" lang="en" loading="lazy"></giscus-widget></div></main><footer class="bg-gray-800 mt-8 py-4 text-gray-300"><div class="container mx-auto flex justify-center text-sm">© 2022 Shariff Faleel</div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"layout":"post","comments":true,"title":"Practices I follow with the machine learning pipeline","tags":["deep learning"],"tagline":"While the ml-pipeline solves some of the problems I encounter, it doesn't solve all of them. Here I describe my process beyond the pipeline."},"content":"\r\n*\u003csmall\u003eEdit [01-01-2019]: Changed the reference to the old mlpipeline to the new\u003c/small\u003e*\r\n\r\nWhile the [ml-pipeline](https://github.com/ahmed-shariff/ml-pipeline) and [experiment log]({{ site.baseurl }}{% post_url 2018-06-11-Experiment-log%}) I use gives me a better control over my machine learning projects, they don't cover all the bases. Here I outline some of the practices I have grown to follow to make my life easier. Note that by no means is this a perfect process. I am simply documenting my current practices here, which I hope can help me as well as others, in the future. I also am listing down some of the problems when working on machine learning in [ML problems I run into]({{ site.baseurl }}{% post_url 2018-07-26-ml-problems%}).\r\n\r\nI generally structure my machine learning project into four directories:\r\n* Datasets: A directory that contains the data and related materials (meta-data, configuration files, data related scripts, etc.)\r\n* Models: A directory that contains the code for the models and related tools. This directory will also contain the output files of the ml-pipeline. \r\n* Deployment: If the models I am working on is expected to go into a production environment, any code related to testing trained models in such context goes in here.\r\n* Data_processing: Sometimes I can't use the data made available straight away. I use tend to write scripts to clean the data, which I store here.\r\n\r\nIn addition the projects root will contain the configuration files used by the ml-pipeline.\r\n\r\nThe function of the directories are divided such that, when training, only the `Datasets` and `Models` directories are used. When working on piecing together the trained model for deployment, only the `Datasets` and `Deployment` directories are used. Likewise, when I am working on the dataset, only the `Datasets` and `Data_processing` directories are used.\r\n\r\nThe above division of the workflow can be thought of as a lose *separation of concerns*. In machine learning defining clear boundaries for components, which then can be improved and tested in isolation like in a traditional software system, is very tricky. While this division of the workflow is not a perfect solution, it works well for me, *for now*. In the remainder of this article I will try to describe how I define and maintain the content of each of this directories (components).\r\n\r\n\r\n## Datasets\r\nThis directory primarily contains the datasets and related content. That is, image files, video files, audio files, stored embedding, etc. This also contains the meta data of the datasets. Occasionally, if there are any tools/scripts, which are used to make necessary changes to the datasets, are also stored here. This allows for the use of a tool like [dvc][https://github.com/iterative/dvc] to maintain the data. In addition I also store any pre-trained weights in this directory. The pre-trained weights are stored here as it can be used in both the training process and deployment systems. As each dataset tends to have it's own annotation/meta data format, I prefer to convert the meta-data to a fixed [format]({{ site.baseurl }}{% post_url 2018-07-26-ml-problems%}#meta-data-models) which eases the work that needs to be done when defining data-loaders. As of now I am using a json format. In the future I'll be moving to a better meta-data model, that pulls from a properly maintained database of features[^fn-coco]. The json format I use for now is as follows:\r\n- class labels: a string, if images contain multiple classes, an array of strings.\r\n  - eg: \r\n\t- Single item: \u003ccode\u003ecar\u003c/code\u003e\r\n\t- Multiple items:\u003ccode\u003e[car, bike, person]\u003c/code\u003e\r\n- counts: similar to the above, but instead of strings, an object with one attribute - the name is a class label, the value is the count of that item.\r\n  - eg: \r\n\t- Single item: \u003ccode\u003e{car:2}\u003c/code\u003e\r\n\t- Multiple items: \u003ccode\u003e[{car:2}, {bike:3}, {person: 10}]\u003c/code\u003e\r\n- Bounding boxes: a list of json objects (pretty much always there is more than one bounding box). Each containing 5 attributes - class labels and the coordinates.\r\n  - eg: \u003ccode\u003e[{name: car, x1:10, x2: 250, y1:22, y2: 320}, \r\n  {name: bike, x1:42, x2: 33, y1:67, y2: 53}]\u003c/code\u003e\r\n- Segmentation mask: a string pointing to the file containing the mask for the respective image.\r\n\r\n## Models\r\nHere I host the models and related content and this is where the core of my work takes place. For each model I am developing I have a separate script. To keep things cleaner, I prepend the id of the primary experiment the model in the script is related to. Also, the pipeline stores the outputs in this directory. This way this directory can be maintained as a separate repository using version control to keep track of the experiments conducted. I have made the habit of making a commit before and after a training session and record the commit's hash in the experiment_log [^fn-commit-feature]\r\n\r\nSome parts of the code in several models scripts can be shared. The code in the script can be divided into two segments:\r\n- Code related to the construction of the model.\r\n- Code related to the training and evaluation process of the model.\r\n\r\nIn most cases it is the second category of code that is shared among the scripts. When I export a model, I [make a copy of the script]({{ site.baseurl }}{% post_url 2018-07-26-ml-problems%}#model-code-and-weight) and store it along with the weights. For an exported model, the training and evaluation code is mostly irrelevant. Hence, only the code that falls into the second category is abstracted to a separate script which can be imported here. Any code that is related to the construction of a model will be available the respective script. As such, there are several components that I have delegated to a different script, such as the data-loaders and implantation details specific to a library (saving and loading weights, setting hyper-parameters, cleaning saved cache of previous iterations, etc.). Note that if using an intermediate format which exports both the weights and the graph (ONNX or tensorflow's *saved models*), this will not be issue [^fn-export-files].\r\n\r\n## Deployment\r\nAnything beyond training, this is the place where I have em all. Once I have a model I am comfortable with, I export it and build the necessary pieces needed to deploy/test the exported model. Generally I use the script I have saved along with the exported model to construct the model and set up other components around it.\r\n\r\n## Data processing\r\nSince 'machine learning', data is always a vital part. Almost always, all projects have new data. Even though deep learning might alleviate the need for feature engineering, the data generally needs to be cleaned and processed before being used for training. While I prefer storing the data itself in the `Datasets` directory, the scripts I use to process the data and the related intermediate states and meta-data, I store in this directory. This is mostly to avoid polluting the `Datasets` directory. This way, the dirty stuff is in here, while the `Datasets` directory will be holding the actual data and meta-data in it. \r\n\r\n## Content in the root directory\r\nThe root directory primarily would contain the configuration files used by the pipeline and the experiment log and the [notes]({{ site.baseurl }}{% post_url 2018-07-26-ml-problems%}#).\r\n\r\n\r\n[^fn-commit-feature]: Hoping to make commiting and recording before and after each experiment a feature in the ml-pipeline.\r\n[^fn-coco]: The coco annotation format is a solution I am looking at for a propper meta-data model.\r\n[^fn-export-files]: An alternate approach I am working on is to export all files loaded/imported from the *models* directory. That is, a copy of the scripts used will be saved along with the model.\r\n"},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"2018-08-01-mlp_file_structure"},"buildId":"o1NKsy16OVz-k8e3hzwe4","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>